{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a96b6d-0d24-4e62-94ea-07eacb4c3c9d",
   "metadata": {},
   "source": [
    "# Swallow System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72253ed8-ba11-4585-88fd-f3115a6cf075",
   "metadata": {},
   "source": [
    "### Queries that focus on: timings, performance, task duration, system state, errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b091258c-c101-4635-a51d-89e8e31f8f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505a583a-7721-4eeb-9626-f737cdc27a25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"SAMBANOVA_API_KEY\"))  # or whatever your variable name is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "57ad170b-8775-4261-b653-5a6815397bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SAMBANOVA_API_KEY\"] = '505a583a-7721-4eeb-9626-f737cdc27a25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2fe0d9eb-428f-4ab2-8a3c-acf8fb340200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowcept import Flowcept\n",
    "from workflow import Workflow\n",
    "from qa_chain import QAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed1e4552-7cad-4439-a416-4759d8f60316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input I = 12\n",
      "I → H: 12 → 43\n",
      "H → E: 43 → 462.25\n",
      "H → F: 43 → 39.344631145812\n",
      "H → G: 43 → 38\n",
      "E → D: 462.25 → 213674.0625\n",
      "F → C: 39.344631145812 → 10.672359527074835\n",
      "G → B: 38 → 234.2477321128211\n",
      "D,C,B → A: (213674.0625, 10.672359527074835, 234.2477321128211) → 71306.32753054664\n",
      "\n",
      "Final Result: I(12) → A(71306.3275)\n",
      "Workflow_id=0e5ae5e3-4f21-4836-a4b9-044b423fde2f\n"
     ]
    }
   ],
   "source": [
    "workflow_id = Workflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f44a7ce4-735c-4cee-867d-9292b967b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_id = '0e5ae5e3-4f21-4836-a4b9-044b423fde2f'\n",
    "qa = QAChain().build(workflow_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0534b3b9-4fcc-4f62-b0ba-8b0514e7fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def cot_prompt(query):\n",
    "    \"\"\"Add Chain of Thought prompting to a query\"\"\" \n",
    "    return f\"Let's think step by step. {query} Please explain your reasoning process.\"\n",
    "\n",
    "def benchmark_query_with_tracking(qa_chain, base_query_id, query, runs=5, use_cot=False, context=None):\n",
    "    \"\"\"\n",
    "    Run multiple queries and automatically track them in the QAChain DataFrame\n",
    "    \n",
    "    Args:\n",
    "        qa_chain: Your QAChain instance\n",
    "        base_query_id: Base ID like \"DF-DL-Q01-NS-NC-R3-Swallow\" (R3 sets starting run number)\n",
    "        query: The actual query text\n",
    "        runs: Number of times to run the query\n",
    "        use_cot: Whether to use Chain of Thought prompting\n",
    "        context: Custom context (if None, uses default based on query_id)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    prompt = cot_prompt(query) if use_cot else query\n",
    "    \n",
    "    # MODIFY QUERY ON COT \n",
    "    if use_cot and \"NS\" in base_query_id:\n",
    "        base_query_id = base_query_id.replace(\"NS\", \"CoT\")\n",
    "    \n",
    "    # EXTRACT STARTING ITERATION NUM\n",
    "    run_match = re.search(r'-R(\\d+)$', base_query_id)  # Match run number only if at end of string\n",
    "    if run_match:\n",
    "        start_run = int(run_match.group(1))\n",
    "        # REMOVE RUN SUFFIX ONLY AT END\n",
    "        base_id_clean = re.sub(r'-R\\d+$', '', base_query_id)\n",
    "    else:\n",
    "        start_run = 1\n",
    "        base_id_clean = base_query_id\n",
    "    \n",
    "    for i in range(runs):\n",
    "        current_run = start_run + i\n",
    "        query_id = f\"{base_id_clean}-R{current_run:02d}\"\n",
    "        \n",
    "       \n",
    "        if context:\n",
    "            result = qa_chain.ask(prompt, query_id=query_id, context=context)\n",
    "        else:\n",
    "            result = qa_chain.ask(prompt, query_id=query_id)\n",
    "        \n",
    "        # RESPONSE AND TIMING INFO\n",
    "        response = result[\"result\"]\n",
    "        query_row = qa_chain.query_df[qa_chain.query_df['Query_ID'] == query_id].iloc[-1]\n",
    "        response_time = query_row['Response_Time']\n",
    "        char_count = query_row['Response_Chars']\n",
    "        \n",
    "        results.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"run\": current_run,\n",
    "            \"response_time_sec\": response_time,\n",
    "            \"response\": response,\n",
    "            \"char_count\": char_count\n",
    "        })\n",
    "    \n",
    "    avg_time = sum(r[\"response_time_sec\"] for r in results) / runs\n",
    "    avg_char_count = sum(r[\"char_count\"] for r in results) / runs\n",
    "    \n",
    "    return {\n",
    "        \"base_query_id\": base_query_id,\n",
    "        \"query\": query,\n",
    "        \"prompt_used\": prompt,\n",
    "        \"use_cot\": use_cot,\n",
    "        \"runs\": results,\n",
    "        \"average_response_time_sec\": avg_time,\n",
    "        \"average_char_count\": avg_char_count\n",
    "    }\n",
    "\n",
    "def run_query_suite(qa_chain, query_suite):\n",
    "    \"\"\"\n",
    "    Run a suite of queries with different configurations\n",
    "    \n",
    "    Args:\n",
    "        qa_chain: Your QAChain instance\n",
    "        query_suite: List of dictionaries with query configurations\n",
    "                    Each dict should have: base_id, query, runs, use_cot, context\n",
    "    \n",
    "    Example:\n",
    "        suite = [\n",
    "            {\n",
    "                \"base_id\": \"DF-DL-Q01-NS-NC-R1-Swallow\",\n",
    "                \"query\": \"What is the data lineage?\",\n",
    "                \"runs\": 3,\n",
    "                \"use_cot\": False,\n",
    "                \"context\": None\n",
    "            },\n",
    "            {\n",
    "                \"base_id\": \"CF-EO-Q02-CoT-FC-R5-Swallow\", \n",
    "                \"query\": \"What is the execution order?\",\n",
    "                \"runs\": 3,\n",
    "                \"use_cot\": True,\n",
    "                \"context\": \"X\"\n",
    "            }\n",
    "        ]\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for config in query_suite:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Running: {config['base_id']}\")\n",
    "        print(f\"Query: {config['query']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = benchmark_query_with_tracking(\n",
    "            qa_chain=qa_chain,\n",
    "            base_query_id=config[\"base_id\"],\n",
    "            query=config[\"query\"],\n",
    "            runs=config.get(\"runs\", 5),\n",
    "            use_cot=config.get(\"use_cot\", False),\n",
    "            context=config.get(\"context\", None)\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        print(f\"Average Response Time: {result['average_response_time_sec']:.2f}s\")\n",
    "        print(f\"Average Character Count: {result['average_char_count']:.0f}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def generate_query_ids_for_batch(base_id, runs, start_run=1):\n",
    "    \"\"\"\n",
    "    Helper function to generate query IDs for batch accuracy updates\n",
    "    \n",
    "    Args:\n",
    "        base_id: Base query ID (without run numbers)\n",
    "        runs: Number of runs\n",
    "        start_run: Starting run number\n",
    "    \n",
    "    Returns:\n",
    "        List of query IDs\n",
    "    \"\"\"\n",
    "    base_id_clean = re.sub(r'-R\\d+$', '', base_id)\n",
    "    \n",
    "    query_ids = []\n",
    "    for i in range(runs):\n",
    "        current_run = start_run + i\n",
    "        query_id = f\"{base_id_clean}-R{current_run:02d}\"\n",
    "        query_ids.append(query_id)\n",
    "    \n",
    "    return query_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f0958097-fadc-4374-a6ac-6382c4967749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Query_ID, Query_Text, Query_Chars, Response_Text, Response_Chars, Response_Time, Accuracy]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# DF TO INSTANCE\n",
    "qa.query_df = pd.DataFrame(columns=[\n",
    "    'Query_ID', 'Query_Text', 'Query_Chars', 'Response_Text', \n",
    "    'Response_Chars', 'Response_Time', 'Accuracy'\n",
    "])\n",
    "\n",
    "# METHODS\n",
    "def update_accuracy(self, query_id, accuracy_score):\n",
    "    mask = self.query_df['Query_ID'] == query_id\n",
    "    if mask.any():\n",
    "        self.query_df.loc[mask, 'Accuracy'] = accuracy_score\n",
    "        print(f\"Updated accuracy for {query_id}: {accuracy_score}\")\n",
    "    else:\n",
    "        print(f\"Query ID {query_id} not found\")\n",
    "\n",
    "def export_queries(self, filename=\"query_results.csv\"):\n",
    "    self.query_df.to_csv(filename, index=False)\n",
    "    print(f\"Query results exported to {filename}\")\n",
    "\n",
    "# METHODS IN INSTANCE\n",
    "import types\n",
    "qa.update_accuracy = types.MethodType(update_accuracy, qa)\n",
    "qa.export_queries = types.MethodType(export_queries, qa)\n",
    "\n",
    "# TEST\n",
    "print(qa.query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7dc60b2f-0905-41af-a3bc-e25cb8be1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "def ask(self, query, query_id=None, context=None):\n",
    "    \"\"\"\n",
    "    Main ask method that can optionally take a query_id parameter\n",
    "    If query_id is not provided, auto-generates one\n",
    "    \"\"\"\n",
    "    # If no query_id provided, auto-generate one\n",
    "    if query_id is None:\n",
    "        query_id = f\"Q_{len(self.query_df) + 1}\"\n",
    "    \n",
    "    if context is None:\n",
    "        context = \"Each document represents a task. All tasks belong to a same workflow execution trace. \"\n",
    "        context += \"The time the task started is stored in the started_at. The time the task ended is stored in the ended_at. The task duration is ended_at - started_at for each task \"\n",
    "    \n",
    "    # Prepare full query text\n",
    "    full_query = f\"{context}. {query}\"\n",
    "    \n",
    "    # Time the query\n",
    "    from time import time\n",
    "    t0 = time()\n",
    "    result = self.qa_chain({\"query\": full_query})\n",
    "    response_time = time() - t0\n",
    "    \n",
    "    # Extract response text\n",
    "    response_text = result[\"result\"]\n",
    "    \n",
    "    # Calculate character counts\n",
    "    query_chars = len(query)\n",
    "    response_chars = len(response_text)\n",
    "    \n",
    "    # Add to tracking DataFrame\n",
    "    import pandas as pd\n",
    "    new_row = {\n",
    "        'Query_ID': query_id,\n",
    "        'Query_Text': query,\n",
    "        'Query_Chars': query_chars,\n",
    "        'Response_Text': response_text,\n",
    "        'Response_Chars': response_chars,\n",
    "        'Response_Time': response_time,\n",
    "        'Accuracy': None  # To be filled manually\n",
    "    }\n",
    "    \n",
    "    self.query_df = pd.concat([self.query_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    print(f\"Q: {query}\")\n",
    "    print(response_text)\n",
    "    print(f\"---------------- I took {response_time:.1f} s to answer this.\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Replace the ask method on your existing instance\n",
    "qa.ask = types.MethodType(ask, qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2db1a-2584-4a53-829a-bc338776c619",
   "metadata": {},
   "source": [
    "# Zero-Shot Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9e563c7b-9696-4a50-94c1-c8820fad6956",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.query_df = qa.query_df.iloc[0:0]  # Deletes all rows, but keeps columns. Able to use to wipe it for whatever reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "43d82a82-16f9-4dbd-b054-73982c340fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query_ID</th>\n",
       "      <th>Query_Text</th>\n",
       "      <th>Query_Chars</th>\n",
       "      <th>Response_Text</th>\n",
       "      <th>Response_Chars</th>\n",
       "      <th>Response_Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query_ID, Query_Text, Query_Chars, Response_Text, Response_Chars, Response_Time, Accuracy]\n",
       "Index: []"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c187cc4-a5f3-4163-8005-3aaa982fe8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To turn df into csv\n",
    "qa.query_df.to_csv(\"query_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07aaae-03d9-4107-b627-7df5a6a95632",
   "metadata": {},
   "source": [
    "### What was in the input value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5ff4496e-9278-4b7e-8ac8-e5a9ab160a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What was in the input value?\n",
      "The input value was **43**. \n",
      "\n",
      "You can find this information in the 'used' section of the first document, under the key 'arg_0'.\n",
      "---------------- I took 6.1 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"What was in the input value?\",query_id = 'DF-DL-Q01-NS-LC-Swallow-R01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "241d80ce-4463-4300-939b-cb813a240bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What was in the input value?\n",
      "The input value was **43**. \n",
      "\n",
      "You can find this information in the 'used' section of the first document, under the key 'arg_0'.\n",
      "---------------- I took 6.3 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"What was in the input value?\",query_id = 'DF-DL-Q01-NS-LC-Swallow-R02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2f009b5f-b17a-4d95-a68c-221c5ce039e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What was in the input value?\n",
      "The input value was **43**. \n",
      "\n",
      "You can find this information in the 'used' section of the first document, under the key 'arg_0'.\n",
      "---------------- I took 6.1 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"What was in the input value?\",query_id = 'DF-DL-Q01-NS-LC-Swallow-R03')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6f1b0-4e7c-41d9-b789-d3c27b8fd606",
   "metadata": {},
   "source": [
    "### How was the initial input I transformed in the first step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cad26048-d16d-4da4-893d-d658fed2dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How was the initial input I transformed in the first step?\n",
      "I don't know.\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"How was the initial input I transformed in the first step?\", query_id = \"DF-DL-Q02-NS-LC-Swallow-R01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3661b352-107c-46c1-a0e8-f73fb81f3e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How was the initial input I transformed in the first step?\n",
      "I don't know.\n",
      "---------------- I took 6.2 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"How was the initial input I transformed in the first step?\", query_id = \"DF-DL-Q02-NS-LC-Swallow-R02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b9f0f24e-3c22-4e02-b2ac-01708c43e012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How was the initial input I transformed in the first step?\n",
      "I don't know.\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"How was the initial input I transformed in the first step?\", query_id = \"DF-DL-Q02-NS-LC-Swallow-R03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a32838-8735-4bcf-b727-eed21cc40182",
   "metadata": {},
   "source": [
    "### What intermediate values were generated from input 'I'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eec27404-a63f-4894-a45a-b1cae60db183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What intermediate values were generated from input 'I'?\n",
      "The intermediate values generated from input 'I' are:\n",
      "\n",
      "* **43** \n",
      "\n",
      "This value is found in the 'generated' field of the document with task_id '1750258167.173979'.\n",
      "---------------- I took 6.4 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"What intermediate values were generated from input 'I'?\", query_id = \"DF-DL-Q03-NS-LC-Swallow-R01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8ec4fe5b-80f2-4476-b0d8-7cf9153ff804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What intermediate values were generated from input 'I'?\n",
      "The intermediate values generated from input 'I' are:\n",
      "\n",
      "* **43** \n",
      "\n",
      "This value is found in the 'generated' field of the document with task_id '1750258167.173979'.\n",
      "---------------- I took 6.2 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"What intermediate values were generated from input 'I'?\", query_id = \"DF-DL-Q03-NS-LC-Swallow-R02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b472492c-9277-4512-ab24-8f65523d80fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What intermediate values were generated from input 'I'?\n",
      "The intermediate values generated from input 'I' are:\n",
      "\n",
      "* **43** \n",
      "\n",
      "This value is found in the 'generated' field of the document with task_id '1750258167.173979'.\n",
      "---------------- I took 6.3 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"What intermediate values were generated from input 'I'?\", query_id = \"DF-DL-Q03-NS-LC-Swallow-R03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081685ef-fc00-486a-99cf-46fdfd2f6441",
   "metadata": {},
   "source": [
    "### What tasks contributed data to the final score 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "196a3c47-f417-426f-a135-89f62e20a9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What tasks contributed data to the final score 'A'?\n",
      "I don't know.\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"What tasks contributed data to the final score 'A'?\", query_id = \"DF-DL-Q08-NS-LC-Swallow-R01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd8408d2-149f-4e8e-926d-cef262e4c4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What tasks contributed data to the final score 'A'?\n",
      "I don't know.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"What tasks contributed data to the final score 'A'?\", query_id = \"DF-DL-Q08-NS-LC-Swallow-R02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b272761a-8d3c-4510-b0d7-a4384e87fb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What tasks contributed data to the final score 'A'?\n",
      "I don't know.\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.ask(\"What tasks contributed data to the final score 'A'?\", query_id = \"DF-DL-Q08-NS-LC-Swallow-R03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b09fa3c9-48cd-4122-aee3-f50c63ffc9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: DF-DL-Q04-NS-LC-Swallow-R01\n",
      "Query: What were the outputs of the tasks 'H_TO_E', 'H_TO_F', and 'H_TO_G'?\n",
      "============================================================\n",
      "Q: What were the outputs of the tasks 'H_TO_E', 'H_TO_F', and 'H_TO_G'?\n",
      "I don't have information about the tasks 'H_TO_E', 'H_TO_F', and 'H_TO_G' in the provided data.\n",
      "---------------- I took 6.2 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: What were the outputs of the tasks 'H_TO_E', 'H_TO_F', and 'H_TO_G'?\n",
      "I don't have information about the tasks 'H_TO_E', 'H_TO_F', and 'H_TO_G' in the provided data.\n",
      "---------------- I took 6.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: What were the outputs of the tasks 'H_TO_E', 'H_TO_F', and 'H_TO_G'?\n",
      "I don't have information about the tasks 'H_TO_E', 'H_TO_F', and 'H_TO_G' in the provided data.\n",
      "---------------- I took 6.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.20s\n",
      "Average Character Count: 95\n",
      "\n",
      "============================================================\n",
      "Running: DF-DL-Q05-NS-LC-Swallow-R01\n",
      "Query: How did the value of 'B' change compared to the value of 'D?\n",
      "============================================================\n",
      "Q: How did the value of 'B' change compared to the value of 'D?\n",
      "I don't know.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: How did the value of 'B' change compared to the value of 'D?\n",
      "I don't know.\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: How did the value of 'B' change compared to the value of 'D?\n",
      "I don't know.\n",
      "---------------- I took 6.6 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.18s\n",
      "Average Character Count: 13\n",
      "\n",
      "============================================================\n",
      "Running: DF-DL-Q06-NS-LC-Swallow-R01\n",
      "Query: Trace the lineage of the final output 'A', back to the input 'I'.\n",
      "============================================================\n",
      "Q: Trace the lineage of the final output 'A', back to the input 'I'.\n",
      "I can't answer this question because I don't have access to the data. I need the data to be provided to me in order to process it and answer your question.\n",
      "---------------- I took 6.2 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Trace the lineage of the final output 'A', back to the input 'I'.\n",
      "I can't answer this question because I don't have access to the data. I need the data to be provided to me in order to process it and answer your question.\n",
      "---------------- I took 10.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Trace the lineage of the final output 'A', back to the input 'I'.\n",
      "I can't answer this question because I don't have access to the data. I need the data to be provided to me in order to process it and answer your question.\n",
      "---------------- I took 6.2 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 7.60s\n",
      "Average Character Count: 155\n",
      "\n",
      "============================================================\n",
      "Running: DF-DL-Q07-NS-LC-Swallow-R01\n",
      "Query: Trace the lineage of 'C', back to the input 'I'.\n",
      "============================================================\n",
      "Q: Trace the lineage of 'C', back to the input 'I'.\n",
      "I can't answer this question because I don't have access to the data.\n",
      "---------------- I took 6.2 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Trace the lineage of 'C', back to the input 'I'.\n",
      "I can't answer this question because I don't have access to the data.\n",
      "---------------- I took 7.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Trace the lineage of 'C', back to the input 'I'.\n",
      "I can't answer this question because I don't have access to the data.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.62s\n",
      "Average Character Count: 69\n"
     ]
    }
   ],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"DF-DL-Q04-NS-LC-Swallow-R01\",\n",
    "        \"query\": \"What were the outputs of the tasks 'H_TO_E', 'H_TO_F', and 'H_TO_G'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-DL-Q05-NS-LC-Swallow-R01\", \n",
    "        \"query\": \"How did the value of 'B' change compared to the value of 'D?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-DL-Q06-NS-LC-Swallow-R01\",\n",
    "        \"query\": \"Trace the lineage of the final output 'A', back to the input 'I'.\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,  # Uses Chain of Thought prompting\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-DL-Q07-NS-LC-Swallow-R01\",\n",
    "        \"query\": \"Trace the lineage of 'C', back to the input 'I'.\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc1386c-9fac-48d3-9ab0-ec371dedc6ef",
   "metadata": {},
   "source": [
    "## Dataflow Transformation logic Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "46c1b2db-1754-4ad8-81a9-bf397f569f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: DF-TL-Q01-NS-LC-Swallow\n",
      "Query: Which functions process data sequentially and which process data in parallel?\n",
      "============================================================\n",
      "Q: Which functions process data sequentially and which process data in parallel?\n",
      "I can't answer that question based on the provided data. \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Lack of Information about Task Relationships:** The data only provides information about individual tasks, but doesn't indicate how they are connected or ordered within the workflow. \n",
      "* **No Indication of Parallelism:** There are no fields or values that explicitly state whether tasks are executed in parallel or sequentially.\n",
      "\n",
      "To determine if tasks are processed sequentially or in parallel, you would need additional information about the workflow structure, such as:\n",
      "\n",
      "* **Dependencies:**  Does one task depend on the completion of another?\n",
      "* **Concurrency Control:** Are there mechanisms in place to allow multiple tasks to run at the same time?\n",
      "* **Workflow Diagram:** A visual representation of the workflow would clearly show the relationships between tasks. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about the data!\n",
      "---------------- I took 7.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 7.41s\n",
      "Average Character Count: 904\n",
      "\n",
      "============================================================\n",
      "Running: DF-TL-Q02-NS-LC-Swallow\n",
      "Query: Is there any data transformation that involves conditional branching within a function?\n",
      "============================================================\n",
      "Q: Is there any data transformation that involves conditional branching within a function?\n",
      "I don't see any data transformations that involve conditional branching within a function in the provided data. \n",
      "\n",
      "The data you've shared appears to be structured logs of task executions, containing information like task IDs, activity IDs, start and end times, resource usage, and generated outputs. \n",
      "\n",
      "There are no indications of functions being applied to this data with conditional logic. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about the data.\n",
      "---------------- I took 7.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.97s\n",
      "Average Character Count: 452\n",
      "\n",
      "============================================================\n",
      "Running: DF-TL-Q03-NS-LC-Swallow\n",
      "Query: Which task applied a logarithmic transformation?\n",
      "============================================================\n",
      "Q: Which task applied a logarithmic transformation?\n",
      "The task that applied a logarithmic transformation is the one with `activity_id` **'F_TO_C'**. \n",
      "\n",
      "This can be inferred from the context provided, which mentions a logarithmic transformation in relation to this activity ID.\n",
      "---------------- I took 6.3 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.26s\n",
      "Average Character Count: 221\n",
      "\n",
      "============================================================\n",
      "Running: DF-TL-Q04-NS-LC-Swallow\n",
      "Query: Are there any tasks that square or root their inputs? Which ones?\n",
      "============================================================\n",
      "Q: Are there any tasks that square or root their inputs? Which ones?\n",
      "I can't tell you which tasks square or root their inputs. \n",
      "\n",
      "The provided data shows the input values for each task, but it doesn't tell me what the tasks actually do with those inputs. To know if a task squares or roots its input, I would need to see the code for that task.\n",
      "---------------- I took 6.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.38s\n",
      "Average Character Count: 274\n",
      "\n",
      "============================================================\n",
      "Running: DF-TL-Q05-NS-LC-Swallow\n",
      "Query: Did any task apply a nonlinear transformation to its input?\n",
      "============================================================\n",
      "Q: Did any task apply a nonlinear transformation to its input?\n",
      "I don't know.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.02s\n",
      "Average Character Count: 13\n",
      "\n",
      "============================================================\n",
      "Running: DF-TL-Q06-NS-LC-Swallow\n",
      "Query: How does 'H_TO_F' process the input from 'H'?\n",
      "============================================================\n",
      "Q: How does 'H_TO_F' process the input from 'H'?\n",
      "I don't know.\n",
      "---------------- I took 6.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.11s\n",
      "Average Character Count: 13\n",
      "\n",
      "============================================================\n",
      "Running: DF-TL-Q07-NS-LC-Swallow\n",
      "Query: How are the inputs of 'E_TO_D', 'F_TO_C', and 'G_TO_B' combined?\n",
      "============================================================\n",
      "Q: How are the inputs of 'E_TO_D', 'F_TO_C', and 'G_TO_B' combined?\n",
      "I don't know.\n",
      "---------------- I took 6.5 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.48s\n",
      "Average Character Count: 13\n",
      "\n",
      "============================================================\n",
      "Running: DF-TL-Q08-NS-LC-Swallow\n",
      "Query: Describe each transformation from task to task.\n",
      "============================================================\n",
      "Q: Describe each transformation from task to task.\n",
      "Based on the provided data, here's a description of the transformations from task to task:\n",
      "\n",
      "1. **DCB_TO_A:** This task takes an input `arg_0` with a value of 213674.0625 and generates an output `arg_0` with a value of 71306.32753054664. This suggests a significant reduction or transformation of the input data.\n",
      "\n",
      "2. **F_TO_C:** This task takes an input `arg_0` with a value of 39.344631145812 and generates an output `arg_0` with a value of 10.672359527074835. This indicates a further reduction or scaling of the input value.\n",
      "\n",
      "3. **E_TO_D:** This task takes an input `arg_0` with a value of 462.25 and generates an output `arg_0` with a value of 213674.0625. This transformation appears to be the inverse of the DCB_TO_A task, suggesting a possible data reconstruction or decoding step.\n",
      "\n",
      "4. **H_TO_G:** This task takes an input `arg_0` with a value of 43 and generates an output `arg_0` with a value of 38. This transformation involves a minor decrease in the input value, potentially indicating a filtering or refinement step.\n",
      "\n",
      "**Overall Workflow:**\n",
      "\n",
      "The sequence of transformations suggests a workflow that involves:\n",
      "\n",
      "- **Data Reduction/Transformation (DCB_TO_A):** The initial task significantly reduces the input data.\n",
      "- **Further Reduction/Scaling (F_TO_C):** The second task further scales down the output from the previous step.\n",
      "- **Data Reconstruction/Decoding (E_TO_D):** The third task reconstructs or decodes the data back to a larger value.\n",
      "- **Refinement/Filtering (H_TO_G):** The final task applies a minor refinement or filtering to the reconstructed data.\n",
      "\n",
      "**Note:** This analysis is based on the limited data provided. A more comprehensive understanding of the workflow would require additional context about the nature of the data and the specific operations performed in each task.\n",
      "---------------- I took 8.2 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 8.20s\n",
      "Average Character Count: 1801\n"
     ]
    }
   ],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"DF-TL-Q01-NS-LC-Swallow\",\n",
    "        \"query\": \"Which functions process data sequentially and which process data in parallel?\",\n",
    "        \"runs\": 1,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-TL-Q02-NS-LC-Swallow\",\n",
    "        \"query\": \"Is there any data transformation that involves conditional branching within a function?\",\n",
    "        \"runs\": 1,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-TL-Q03-NS-LC-Swallow\",\n",
    "        \"query\": \"Which task applied a logarithmic transformation?\",\n",
    "        \"runs\": 1,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-TL-Q04-NS-LC-Swallow\",\n",
    "        \"query\": \"Are there any tasks that square or root their inputs? Which ones?\",\n",
    "        \"runs\": 1,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-TL-Q05-NS-LC-Swallow\",\n",
    "        \"query\": \"Did any task apply a nonlinear transformation to its input?\",\n",
    "        \"runs\": 1,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-TL-Q06-NS-LC-Swallow\", \n",
    "        \"query\": \"How does 'H_TO_F' process the input from 'H'?\",\n",
    "        \"runs\": 1,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-TL-Q07-NS-LC-Swallow\",\n",
    "        \"query\": \"How are the inputs of 'E_TO_D', 'F_TO_C', and 'G_TO_B' combined?\",\n",
    "        \"runs\": 1,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-TL-Q08-NS-LC-Swallow\",\n",
    "        \"query\": \"Describe each transformation from task to task.\",\n",
    "        \"runs\": 1,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4726e888-c7ba-4919-b5e6-b1f6fbf3d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load existing CSV\n",
    "existing_df = pd.read_csv(\"query_data.csv\")\n",
    "\n",
    "# New data to overwrite first 15 rows\n",
    "new_df = pd.DataFrame({\n",
    "    'id': ['NEW-Q01'],\n",
    "    'question': ['What is the new question?'],\n",
    "    'answer': ['42'],\n",
    "    'response_time': [5.678]\n",
    "    # You can add more rows if needed\n",
    "})\n",
    "\n",
    "# Ensure all columns match between existing_df and new_df\n",
    "all_columns = existing_df.columns.union(new_df.columns)\n",
    "existing_df = existing_df.reindex(columns=all_columns)\n",
    "new_df = new_df.reindex(columns=all_columns)\n",
    "\n",
    "# Overwrite rows 0–14 (i.e., the first 15 rows) with as many rows as new_df has\n",
    "num_rows_to_replace = min(len(new_df), 15)\n",
    "existing_df.iloc[:num_rows_to_replace] = new_df.iloc[:num_rows_to_replace].values\n",
    "\n",
    "# Save back to CSV\n",
    "existing_df.to_csv(\"query_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a4c2e-700e-484e-a5bb-fc1739773923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broke due to token limit. It got 2/3 of the iterations for the final query in this set. Just throw in another API key to fix this for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94d7dc-6466-48ee-80a8-377a9c9aa6d8",
   "metadata": {},
   "source": [
    "## Dataflow: Inputs and Outputs of Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e0d6a6e4-8132-448d-bf66-a30f355acc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: DF-INOP-Q02-NS-LC-R1-Swallow\n",
      "Query: Was the output of I_TO_H used directly by multiple downstream tasks? If so what tasks?\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "('Sambanova /complete call failed with status code 429.', '{\"error\":{\"code\":null,\"message\":\"Rate limit exceeded\",\"param\":null,\"type\":\"rate_limit_exceeded\"}}\\n.')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[153]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m      1\u001b[39m query_suite = [\n\u001b[32m      2\u001b[39m   \u001b[38;5;66;03m#  {\u001b[39;00m\n\u001b[32m      3\u001b[39m    \u001b[38;5;66;03m#     \"base_id\": \"DF-INOP-Q01-NS-LC-R1-Swallow\",\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     }\n\u001b[32m     58\u001b[39m ]\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Run all queries in the suite\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m all_results = \u001b[43mrun_query_suite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_suite\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[130]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mrun_query_suite\u001b[39m\u001b[34m(qa_chain, query_suite)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m result = \u001b[43mbenchmark_query_with_tracking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqa_chain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqa_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_query_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbase_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mruns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_cot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m all_results.append(result)\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAverage Response Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33maverage_response_time_sec\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[130]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mbenchmark_query_with_tracking\u001b[39m\u001b[34m(qa_chain, base_query_id, query, runs, use_cot, context)\u001b[39m\n\u001b[32m     45\u001b[39m     result = qa_chain.ask(prompt, query_id=query_id, context=context)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     result = \u001b[43mqa_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Get response and timing info from qa_chain's DataFrame\u001b[39;00m\n\u001b[32m     50\u001b[39m response = result[\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mask\u001b[39m\u001b[34m(self, query, query_id, context)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[32m     21\u001b[39m t0 = time()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqa_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_query\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m response_time = time() - t0\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Extract response text\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/retrieval_qa/base.py:154\u001b[39m, in \u001b[36mBaseRetrievalQA._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    153\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m._get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_source_documents:\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m.output_key: answer, \u001b[33m\"\u001b[39m\u001b[33msource_documents\u001b[39m\u001b[33m\"\u001b[39m: docs}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:608\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[32m0\u001b[39m], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    604\u001b[39m         _output_key\n\u001b[32m    605\u001b[39m     ]\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    609\u001b[39m         _output_key\n\u001b[32m    610\u001b[39m     ]\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    613\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    614\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    615\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m but none were provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    616\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/combine_documents/base.py:138\u001b[39m, in \u001b[36mBaseCombineDocumentsChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[32m    137\u001b[39m other_keys = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[38;5;28mself\u001b[39m.input_key}\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m output, extra_return_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_keys\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m extra_return_dict[\u001b[38;5;28mself\u001b[39m.output_key] = output\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/combine_documents/stuff.py:259\u001b[39m, in \u001b[36mStuffDocumentsChain.combine_docs\u001b[39m\u001b[34m(self, docs, callbacks, **kwargs)\u001b[39m\n\u001b[32m    257\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m._get_inputs(docs, **kwargs)\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/llm.py:319\u001b[39m, in \u001b[36mLLMChain.predict\u001b[39m\u001b[34m(self, callbacks, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[32m    306\u001b[39m \n\u001b[32m    307\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    317\u001b[39m \u001b[33;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m.output_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/llm.py:127\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    124\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    125\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/llm.py:139\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    137\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    147\u001b[39m         cast(\u001b[38;5;28mlist\u001b[39m, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    148\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_sambanova/chat_models.py:1055\u001b[39m, in \u001b[36mChatSambaNovaCloud._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1053\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[32m   1054\u001b[39m messages_dicts = _create_message_dicts(messages)\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m message = \u001b[38;5;28mself\u001b[39m._process_response(response)\n\u001b[32m   1057\u001b[39m generation = ChatGeneration(\n\u001b[32m   1058\u001b[39m     message=message,\n\u001b[32m   1059\u001b[39m     generation_info={\n\u001b[32m   1060\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfinish_reason\u001b[39m\u001b[33m\"\u001b[39m: message.response_metadata[\u001b[33m\"\u001b[39m\u001b[33mfinish_reason\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1061\u001b[39m     },\n\u001b[32m   1062\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_sambanova/chat_models.py:826\u001b[39m, in \u001b[36mChatSambaNovaCloud._handle_request\u001b[39m\u001b[34m(self, messages_dicts, stop, streaming, **kwargs)\u001b[39m\n\u001b[32m    815\u001b[39m response = http_session.post(\n\u001b[32m    816\u001b[39m     \u001b[38;5;28mself\u001b[39m.sambanova_url,\n\u001b[32m    817\u001b[39m     headers={\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     stream=streaming,\n\u001b[32m    824\u001b[39m )\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    827\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSambanova /complete call failed with status code \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    828\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    829\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    830\u001b[39m     )\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[31mRuntimeError\u001b[39m: ('Sambanova /complete call failed with status code 429.', '{\"error\":{\"code\":null,\"message\":\"Rate limit exceeded\",\"param\":null,\"type\":\"rate_limit_exceeded\"}}\\n.')"
     ]
    }
   ],
   "source": [
    "query_suite = [\n",
    "  #  {\n",
    "   #     \"base_id\": \"DF-INOP-Q01-NS-LC-R1-Swallow\",\n",
    "    #    \"query\": \"What tasks take the value 'H' as input?\",\n",
    "     #   \"runs\": 3,\n",
    "      #  \"use_cot\": False,\n",
    "       # \"context\": None\n",
    "   # },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q02-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Was the output of I_TO_H used directly by multiple downstream tasks? If so what tasks?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q03-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which tasks produce intermediate values that are consumed by other tasks?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q04-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How many values are combined to produce the final output 'A'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q05-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Did any task produce more than one output?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q06-NS-LC-R1-Swallow\", \n",
    "        \"query\": \"What is the data type of the output produced by 'I_TO_H'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q07-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What are the output values of 'E_TO_D', 'F_TO_C', and 'G_TO_B'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q08-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How can 'H_TO_G's output change given the input?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1e36b36b-efa0-4468-84d0-b6054bc5ecb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query_ID</th>\n",
       "      <th>Query_Text</th>\n",
       "      <th>Query_Chars</th>\n",
       "      <th>Response_Text</th>\n",
       "      <th>Response_Chars</th>\n",
       "      <th>Response_Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>DF-TL-Q04-NS-LC-Swallow-R01</td>\n",
       "      <td>Are there any tasks that square or root their ...</td>\n",
       "      <td>65</td>\n",
       "      <td>I can't tell you which tasks square or root th...</td>\n",
       "      <td>274</td>\n",
       "      <td>6.375111</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>DF-TL-Q05-NS-LC-Swallow-R01</td>\n",
       "      <td>Did any task apply a nonlinear transformation ...</td>\n",
       "      <td>59</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>13</td>\n",
       "      <td>6.017566</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>DF-TL-Q06-NS-LC-Swallow-R01</td>\n",
       "      <td>How does 'H_TO_F' process the input from 'H'?</td>\n",
       "      <td>45</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>13</td>\n",
       "      <td>6.108374</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>DF-TL-Q07-NS-LC-Swallow-R01</td>\n",
       "      <td>How are the inputs of 'E_TO_D', 'F_TO_C', and ...</td>\n",
       "      <td>64</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>13</td>\n",
       "      <td>6.481077</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>DF-TL-Q08-NS-LC-Swallow-R01</td>\n",
       "      <td>Describe each transformation from task to task.</td>\n",
       "      <td>47</td>\n",
       "      <td>Based on the provided data, here's a descripti...</td>\n",
       "      <td>1801</td>\n",
       "      <td>8.203693</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Query_ID  \\\n",
       "48  DF-TL-Q04-NS-LC-Swallow-R01   \n",
       "49  DF-TL-Q05-NS-LC-Swallow-R01   \n",
       "50  DF-TL-Q06-NS-LC-Swallow-R01   \n",
       "51  DF-TL-Q07-NS-LC-Swallow-R01   \n",
       "52  DF-TL-Q08-NS-LC-Swallow-R01   \n",
       "\n",
       "                                           Query_Text Query_Chars  \\\n",
       "48  Are there any tasks that square or root their ...          65   \n",
       "49  Did any task apply a nonlinear transformation ...          59   \n",
       "50      How does 'H_TO_F' process the input from 'H'?          45   \n",
       "51  How are the inputs of 'E_TO_D', 'F_TO_C', and ...          64   \n",
       "52    Describe each transformation from task to task.          47   \n",
       "\n",
       "                                        Response_Text Response_Chars  \\\n",
       "48  I can't tell you which tasks square or root th...            274   \n",
       "49                                      I don't know.             13   \n",
       "50                                      I don't know.             13   \n",
       "51                                      I don't know.             13   \n",
       "52  Based on the provided data, here's a descripti...           1801   \n",
       "\n",
       "    Response_Time Accuracy  \n",
       "48       6.375111     None  \n",
       "49       6.017566     None  \n",
       "50       6.108374     None  \n",
       "51       6.481077     None  \n",
       "52       8.203693     None  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.query_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b09bfa-297d-4ef8-a601-f06063acde6e",
   "metadata": {},
   "source": [
    "## Dataflow: Function Level Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72ab2ba9-1c78-4dad-85c6-5c4ae3613f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: DF-FLT-Q01-NS-LC-R1-Swallow\n",
      "Query: What is the role of the 'DCB_TO_A' function in data-flow?\n",
      "============================================================\n",
      "Q: What is the role of the 'DCB_TO_A' function in data-flow?\n",
      "I don't know.\n",
      "---------------- I took 6.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: What is the role of the 'DCB_TO_A' function in data-flow?\n",
      "I don't know.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: What is the role of the 'DCB_TO_A' function in data-flow?\n",
      "I don't know.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.13s\n",
      "Average Character Count: 13\n",
      "\n",
      "============================================================\n",
      "Running: DF-FLT-Q02-NS-LC-R1-Swallow\n",
      "Query: How does the task 'H_TO_G' alter downstream values?\n",
      "============================================================\n",
      "Q: How does the task 'H_TO_G' alter downstream values?\n",
      "I don't have enough information to answer that question. \n",
      "\n",
      "The provided documents contain information about tasks, their execution times, and some system metrics, but they don't explain how tasks interact with each other or how the output of one task influences the input of another. \n",
      "\n",
      "To understand how 'H_TO_G' alters downstream values, I would need:\n",
      "\n",
      "* **Information about the workflow structure:** A description of how tasks are connected and the data flow between them.\n",
      "* **Details about the tasks' functionality:** What each task does and how it processes its input to produce output.\n",
      "* **The specific output of 'H_TO_G':** Knowing the values produced by 'H_TO_G' is crucial to understanding its impact on downstream tasks. \n",
      "\n",
      "\n",
      "Without this context, it's impossible to determine how 'H_TO_G' alters downstream values.\n",
      "---------------- I took 6.8 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: How does the task 'H_TO_G' alter downstream values?\n",
      "I don't have enough information to answer that question. \n",
      "\n",
      "The provided documents contain information about tasks, their execution times, and some system metrics, but they don't explain how tasks interact with each other or how the output of one task influences the input of another. \n",
      "\n",
      "To understand how 'H_TO_G' alters downstream values, I would need:\n",
      "\n",
      "* **Information about the workflow structure:** A description of how tasks are connected and the data flow between them.\n",
      "* **Details about the tasks' functionality:** What each task does and how it processes its input to produce output.\n",
      "* **The specific output of 'H_TO_G':** Knowing the values produced by 'H_TO_G' is crucial to understanding its impact on downstream tasks. \n",
      "\n",
      "\n",
      "Without this context, it's impossible to determine how 'H_TO_G' alters downstream values.\n",
      "---------------- I took 6.9 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: How does the task 'H_TO_G' alter downstream values?\n",
      "I don't have enough information to answer that question. \n",
      "\n",
      "The provided documents contain information about tasks, their execution times, and some system metrics, but they don't explain how tasks interact with each other or how the output of one task influences the input of another. \n",
      "\n",
      "To understand how 'H_TO_G' alters downstream values, I would need:\n",
      "\n",
      "* **Information about the workflow structure:** A description of how tasks are connected and the data flow between them.\n",
      "* **Details about the tasks' functionality:** What each task does and how it processes its input to produce output.\n",
      "* **The specific output of 'H_TO_G':** Knowing the values produced by 'H_TO_G' is crucial to understanding its impact on downstream tasks. \n",
      "\n",
      "\n",
      "Without this context, it's impossible to determine how 'H_TO_G' alters downstream values.\n",
      "---------------- I took 7.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.96s\n",
      "Average Character Count: 822\n",
      "\n",
      "============================================================\n",
      "Running: DF-FLT-Q03-NS-LC-R1-Swallow\n",
      "Query: Are there any functions that perform conditional branching or dynamic transformations?\n",
      "============================================================\n",
      "Q: Are there any functions that perform conditional branching or dynamic transformations?\n",
      "I don't see any functions that perform conditional branching or dynamic transformations in the provided data. \n",
      "\n",
      "The data appears to be structured logs of task executions, containing information about the task's input, output, resource usage, and timing. \n",
      "\n",
      "There are no indications of control flow structures like if-else statements or loops, nor are there any signs of data being modified based on conditions or rules.\n",
      "---------------- I took 6.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Are there any functions that perform conditional branching or dynamic transformations?\n",
      "I don't see any functions that perform conditional branching or dynamic transformations in the provided data. \n",
      "\n",
      "The data appears to be structured logs of task executions, containing information about the task's input, output, resource usage, and timing. \n",
      "\n",
      "There are no indications of control flow structures like if-else statements or loops, nor are there any signs of data being modified based on conditions or rules.\n",
      "---------------- I took 6.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Are there any functions that perform conditional branching or dynamic transformations?\n",
      "I don't see any functions that perform conditional branching or dynamic transformations in the provided data. \n",
      "\n",
      "The data appears to be structured logs of task executions, containing information about the task's input, output, resource usage, and timing. \n",
      "\n",
      "There are no indications of control flow structures like if-else statements or loops, nor are there any signs of data being modified based on conditions or rules.\n",
      "---------------- I took 6.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.40s\n",
      "Average Character Count: 418\n",
      "\n",
      "============================================================\n",
      "Running: DF-FLT-Q04-NS-LC-R1-Swallow\n",
      "Query: How many tasks/ functions does the data flow pass through before the final output?\n",
      "============================================================\n",
      "Q: How many tasks/ functions does the data flow pass through before the final output?\n",
      "Based on the provided data, the workflow execution trace passes through **4 tasks/functions** before the final output. \n",
      "\n",
      "Here's how I arrived at this conclusion:\n",
      "\n",
      "* Each document represents a task within the workflow.\n",
      "* The `activity_id` field in each document indicates the specific function or step being executed (e.g., 'F_TO_C', 'DCB_TO_A', 'E_TO_D', 'I_TO_H').\n",
      "* By examining the unique `activity_id` values across the documents, we can identify the sequence of tasks:\n",
      "\n",
      "1. **F_TO_C**\n",
      "2. **DCB_TO_A**\n",
      "3. **E_TO_D**\n",
      "4. **I_TO_H** \n",
      "\n",
      "Therefore, the data flow passes through these 4 tasks/functions before reaching the final output.\n",
      "---------------- I took 6.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: How many tasks/ functions does the data flow pass through before the final output?\n",
      "Based on the provided data, the workflow execution trace passes through **4 tasks/functions** before the final output. \n",
      "\n",
      "Here's how I arrived at this conclusion:\n",
      "\n",
      "* Each document represents a task within the workflow.\n",
      "* The `activity_id` field in each document indicates the specific function or step being executed (e.g., 'F_TO_C', 'DCB_TO_A', 'E_TO_D', 'I_TO_H').\n",
      "* By examining the unique `activity_id` values across the documents, we can identify the sequence of tasks:\n",
      "\n",
      "1. **F_TO_C**\n",
      "2. **DCB_TO_A**\n",
      "3. **E_TO_D**\n",
      "4. **I_TO_H** \n",
      "\n",
      "Therefore, the data flow passes through these 4 tasks/functions before reaching the final output.\n",
      "---------------- I took 6.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: How many tasks/ functions does the data flow pass through before the final output?\n",
      "Based on the provided data, the workflow execution trace passes through **4 tasks/functions** before the final output. \n",
      "\n",
      "Here's how I arrived at this conclusion:\n",
      "\n",
      "* Each document represents a task within the workflow.\n",
      "* The `activity_id` field in each document indicates the specific function or step being executed (e.g., 'F_TO_C', 'DCB_TO_A', 'E_TO_D', 'I_TO_H').\n",
      "* By examining the unique `activity_id` values across the documents, we can identify the sequence of tasks:\n",
      "\n",
      "1. **F_TO_C**\n",
      "2. **DCB_TO_A**\n",
      "3. **E_TO_D**\n",
      "4. **I_TO_H** \n",
      "\n",
      "Therefore, the data flow passes through these 4 tasks/functions before reaching the final output.\n",
      "---------------- I took 6.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.67s\n",
      "Average Character Count: 632\n",
      "\n",
      "============================================================\n",
      "Running: DF-FLT-Q05-NS-LC-R1-Swallow\n",
      "Query: Which functions produce outputs that are inputs to more than one downstream task?\n",
      "============================================================\n",
      "Q: Which functions produce outputs that are inputs to more than one downstream task?\n",
      "I don't know.\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Which functions produce outputs that are inputs to more than one downstream task?\n",
      "I don't know.\n",
      "---------------- I took 5.8 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Which functions produce outputs that are inputs to more than one downstream task?\n",
      "I don't know.\n",
      "---------------- I took 7.8 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.48s\n",
      "Average Character Count: 13\n",
      "\n",
      "============================================================\n",
      "Running: DF-FLT-Q06-NS-LC-R1-Swallow\n",
      "Query: What relationship exists between the output of 'H_TO_E' and the input 'E_TO_D'?\n",
      "============================================================\n",
      "Q: What relationship exists between the output of 'H_TO_E' and the input 'E_TO_D'?\n",
      "The output of 'H_TO_E' is the input to 'E_TO_D'.\n",
      "---------------- I took 6.3 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: What relationship exists between the output of 'H_TO_E' and the input 'E_TO_D'?\n",
      "The output of 'H_TO_E' is the input to 'E_TO_D'.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: What relationship exists between the output of 'H_TO_E' and the input 'E_TO_D'?\n",
      "The output of 'H_TO_E' is the input to 'E_TO_D'.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 6.09s\n",
      "Average Character Count: 48\n",
      "\n",
      "============================================================\n",
      "Running: DF-FLT-Q07-NS-LC-R1-Swallow\n",
      "Query: Which functions handle parallel data transformations?\n",
      "============================================================\n",
      "Q: Which functions handle parallel data transformations?\n",
      "I don't know.\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Which functions handle parallel data transformations?\n",
      "I don't know.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Which functions handle parallel data transformations?\n",
      "I don't know.\n",
      "---------------- I took 6.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 5.98s\n",
      "Average Character Count: 13\n",
      "\n",
      "============================================================\n",
      "Running: DF-FLT-Q08-NS-LC-R1-Swallow\n",
      "Query: Which functions process data sequentially, and which process data in parallel?\n",
      "============================================================\n",
      "Q: Which functions process data sequentially, and which process data in parallel?\n",
      "I don't know.\n",
      "---------------- I took 5.8 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Which functions process data sequentially, and which process data in parallel?\n",
      "I don't know.\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Which functions process data sequentially, and which process data in parallel?\n",
      "I don't know.\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Average Response Time: 5.88s\n",
      "Average Character Count: 13\n"
     ]
    }
   ],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"DF-FLT-Q01-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What is the role of the 'DCB_TO_A' function in data-flow?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-FLT-Q02-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How does the task 'H_TO_G' alter downstream values?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-FLT-Q03-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Are there any functions that perform conditional branching or dynamic transformations?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-FLT-Q04-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How many tasks/ functions does the data flow pass through before the final output?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-FLT-Q05-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which functions produce outputs that are inputs to more than one downstream task?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-FLT-Q06-NS-LC-R1-Swallow\", \n",
    "        \"query\": \"What relationship exists between the output of 'H_TO_E' and the input 'E_TO_D'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-FLT-Q07-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which functions handle parallel data transformations?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-FLT-Q08-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which functions process data sequentially, and which process data in parallel?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50badbb-6e4a-42f0-afbf-49c52f246c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Run on Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92427e66-b730-4596-b268-dbbbf6cceb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monday, Finish NS Queries. Put them all in a csv from this df so I can better analyze them.\n",
    "# Begin / Finish FS and CoT. Ask Renan about if I should do FS-WIK-WCON-ROLE | FS-NOWIK-WCON-ROLE | FS-NOWIK-NOWCON-ROLE | FS-WIK-NOWCON-NOROLE | FS-WIK-WCON-NOROLE type of thing or is that to complicated. \n",
    "# Also ask him if I should do a true no-shot as in NO CONTEXT AT all, as these above were ran with limited context being what he wrote in Flowcept. Would that be worth doing?\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6c765-7e0d-45d1-a8ab-08b601f4d019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcf83509-bd9c-4f04-bd1c-6103350738da",
   "metadata": {},
   "source": [
    "# Scheduling Data Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c29b0658-de8b-44ed-8349-8093dc83b567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: S-NA-Q01-NS-LC-R1-Swallow\n",
      "Query: On which node or machine did the task G_TO_B run?\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "('Sambanova /complete call failed with status code 429.', '{\"error\":{\"code\":null,\"message\":\"Rate limit exceeded\",\"param\":null,\"type\":\"rate_limit_exceeded\"}}\\n.')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m      1\u001b[39m query_suite = [\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbase_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mS-NA-Q01-NS-LC-R1-Swallow\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     }\n\u001b[32m     58\u001b[39m ]\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Run all queries in the suite\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m all_results = \u001b[43mrun_query_suite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_suite\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mrun_query_suite\u001b[39m\u001b[34m(qa_chain, query_suite)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m result = \u001b[43mbenchmark_query_with_tracking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqa_chain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqa_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_query_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbase_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mruns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_cot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m all_results.append(result)\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAverage Response Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33maverage_response_time_sec\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mbenchmark_query_with_tracking\u001b[39m\u001b[34m(qa_chain, base_query_id, query, runs, use_cot, context)\u001b[39m\n\u001b[32m     46\u001b[39m     result = qa_chain.ask(prompt, query_id=query_id, context=context)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     result = \u001b[43mqa_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# GET RESPONSE AND TIMING \u001b[39;00m\n\u001b[32m     51\u001b[39m response = result[\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mask\u001b[39m\u001b[34m(self, query, query_id, context)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[32m     21\u001b[39m t0 = time()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqa_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_query\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m response_time = time() - t0\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Extract response text\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/retrieval_qa/base.py:154\u001b[39m, in \u001b[36mBaseRetrievalQA._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    153\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m._get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_source_documents:\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m.output_key: answer, \u001b[33m\"\u001b[39m\u001b[33msource_documents\u001b[39m\u001b[33m\"\u001b[39m: docs}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:608\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[32m0\u001b[39m], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    604\u001b[39m         _output_key\n\u001b[32m    605\u001b[39m     ]\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    609\u001b[39m         _output_key\n\u001b[32m    610\u001b[39m     ]\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    613\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    614\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    615\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m but none were provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    616\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/combine_documents/base.py:138\u001b[39m, in \u001b[36mBaseCombineDocumentsChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[32m    137\u001b[39m other_keys = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[38;5;28mself\u001b[39m.input_key}\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m output, extra_return_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_keys\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m extra_return_dict[\u001b[38;5;28mself\u001b[39m.output_key] = output\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/combine_documents/stuff.py:259\u001b[39m, in \u001b[36mStuffDocumentsChain.combine_docs\u001b[39m\u001b[34m(self, docs, callbacks, **kwargs)\u001b[39m\n\u001b[32m    257\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m._get_inputs(docs, **kwargs)\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/llm.py:319\u001b[39m, in \u001b[36mLLMChain.predict\u001b[39m\u001b[34m(self, callbacks, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[32m    306\u001b[39m \n\u001b[32m    307\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    317\u001b[39m \u001b[33;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m.output_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/llm.py:127\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    124\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    125\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain/chains/llm.py:139\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    137\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    147\u001b[39m         cast(\u001b[38;5;28mlist\u001b[39m, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    148\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_sambanova/chat_models.py:1055\u001b[39m, in \u001b[36mChatSambaNovaCloud._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1053\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[32m   1054\u001b[39m messages_dicts = _create_message_dicts(messages)\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m message = \u001b[38;5;28mself\u001b[39m._process_response(response)\n\u001b[32m   1057\u001b[39m generation = ChatGeneration(\n\u001b[32m   1058\u001b[39m     message=message,\n\u001b[32m   1059\u001b[39m     generation_info={\n\u001b[32m   1060\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfinish_reason\u001b[39m\u001b[33m\"\u001b[39m: message.response_metadata[\u001b[33m\"\u001b[39m\u001b[33mfinish_reason\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1061\u001b[39m     },\n\u001b[32m   1062\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/langchain_sambanova/chat_models.py:826\u001b[39m, in \u001b[36mChatSambaNovaCloud._handle_request\u001b[39m\u001b[34m(self, messages_dicts, stop, streaming, **kwargs)\u001b[39m\n\u001b[32m    815\u001b[39m response = http_session.post(\n\u001b[32m    816\u001b[39m     \u001b[38;5;28mself\u001b[39m.sambanova_url,\n\u001b[32m    817\u001b[39m     headers={\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     stream=streaming,\n\u001b[32m    824\u001b[39m )\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    827\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSambanova /complete call failed with status code \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    828\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    829\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    830\u001b[39m     )\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[31mRuntimeError\u001b[39m: ('Sambanova /complete call failed with status code 429.', '{\"error\":{\"code\":null,\"message\":\"Rate limit exceeded\",\"param\":null,\"type\":\"rate_limit_exceeded\"}}\\n.')"
     ]
    }
   ],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"S-NA-Q01-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"On which node or machine did the task G_TO_B run?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"S-NA-Q02-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Where was the task DCB_TO_A executed?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"S-NA-Q03-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Did all the tasks in the workflow run on the same node?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"S-NA-Q04-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How long did each task wait before it was scheduled?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"S-NA-Q05-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which task had the longest delay between being ready and being scheduled?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"S-NA-Q06-NS-LC-R1-Swallow\", \n",
    "        \"query\": \"Where there any idle periods on any node during execution?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"S-NA-Q07-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Did the scheduler assign any tasks to the same resource consecutively?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"S-NA-Q08-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Were any tasks exeucted in parllel across different processes?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a03008-94d0-41ec-9d8f-8a378d8c3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to change API KEY TO RUN AGAIN. RAN OUTTA TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93ac4d-d705-440d-8b80-a129e4679bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "835f8bc2-5ace-4da8-b15e-d66bcc4dd96a",
   "metadata": {},
   "source": [
    "# Telemetry Queries: Task Duration & Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d8249-fd1d-4479-8a86-f0be62dda3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"T-TDT-Q01-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How long did the 'F_TO_C' task take to execute?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-TDT-Q02-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Order the tasks from longest to shortest execution time.\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-TDT-Q03-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which task had the largest influence on overall runtime?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-TDT-Q04-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What was the runtime of the entire workflow?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-TDT-Q05-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which stage of the workflow was most time-consuming?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-TDT-Q06-NS-LC-R1-Swallow\", \n",
    "        \"query\": \"What is the standard deviation of task durations across the workflow?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-TDT-Q07-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What was the average duration of tasks in the parallel stage?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-TDT-Q08-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How much time elapsed between 'I_TO_H' and 'G_TO_B'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44d053-7e2d-40da-9f64-07b39e0d91a6",
   "metadata": {},
   "source": [
    "# Resource Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630a1e8-b298-4249-a3a9-d1659516a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"T-RU-Q01-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What was the peak memory usage during the task 'G_TO_B'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-RU-Q02-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What task had the highest CPU utilization?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-RU-Q03-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which task used the most RAM?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-RU-Q04-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What were the resource usage metrics for 'E_TO_D'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-RU-Q05-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Did any tasks exceed their allocated memory limits?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-RU-Q06-NS-LC-R1-Swallow\", \n",
    "        \"query\": \"Did any task experience memory spikes?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-RU-Q07-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Was the system under high load during the execution of the parallel stage?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-RU-Q08-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"I want the mean, median, and mode of CPU usage across the workflow by task.\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04369d8-7179-4fa3-b549-17c74b82945c",
   "metadata": {},
   "source": [
    "# Errors & Execeptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b08cf-4c37-48c8-bdde-cb8e6a60c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"T-EE-Q01-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Did any task fail with an exception during execution?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-EE-Q02-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Were any runtime errors logged during the execution of the workflow?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-EE-Q03-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What exception occured in the 'H_TO_G' function?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-EE-Q04-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Did any function raise a ValueError or TypeError?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-EE-Q05-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which task caused the workflow to terminate early?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-EE-Q06-NS-LC-R1-Swallow\", \n",
    "        \"query\": \"Were any partial results produced despite an error in one branch?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-EE-Q07-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Were any downstream tasks skipped due to a failure upstream?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"T-EE-Q08-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How many tasks reported failures in the telemetry logs, and what were their associated error messages?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acff53e-c799-4752-839c-8d22f8663cce",
   "metadata": {},
   "source": [
    "# ControlFlow: Execution Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965663d2-7fda-4249-8427-1cbf08452c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"CF-EO-Q01-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What was the first task executed in the workflow?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-EO-Q02-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What was the final task executed in the workflow?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-EO-Q03-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What step was executed after 'I_TO_H'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-EO-Q04-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Did 'F_TO_C' execute before or after 'G_TO_B?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-EO-Q05-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"In what order were D,C, and B produced?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-EO-Q06-NS-LC-R1-Swallow\", \n",
    "        \"query\": \"List the complete execution order of all tasks in the workflow?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-EO-Q07-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Were any steps executed more than once? \",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-EO-Q08-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which steps ran in parallel after H was computed?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b54f5-151d-4107-8966-f1f5ea5da54e",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22ce9c-9522-46f3-94a6-d7d857e2d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"CF-D-Q01-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What are the direct predecessors of 'I_TO_H'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-D-Q02-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which task(s) could not start until 'H_TO_F' was complete?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-D-Q03-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which earlier task did 'DCB_TO_A' depend on?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-D-Q04-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which task triggered the execution of 'H_TO_E'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-D-Q05-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Was 'E_TO_D' apart of a sequential or branched path?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-D-Q06-NS-LC-R1-Swallow\", \n",
    "        \"query\": \"How is 'G_TO_B' related to 'I_TO_H' in the control-flow path?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-D-Q07-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What dependencies exist between the H node and the final node result of A? \",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-D-Q08-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How many tasks depend on another task?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0cffc3-9826-402a-8cbf-80838a40c243",
   "metadata": {},
   "source": [
    "# Failure & Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c2462-ee4a-4938-b22b-00c4e81242dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"CF-FEH-Q01-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Did any task in the workflow fail?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-FEH-Q02-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Did any tasks raise an error during execution?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-FEH-Q03-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Were any tasks skipped entirely?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-FEH-Q04-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Was the workflow able to complete successfully despite any failure?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-FEH-Q05-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"If 'H_TO_G' had failed, what downstream tasks would have been affected?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-FEH-Q06-NS-LC-R1-Swallow\", \n",
    "        \"query\": \"What tasks failed and what were their immediate predecessors?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-FEH-Q07-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What is the reason that 'H_TO_F' would fail?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"CF-FEH-Q08-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Are there any tasks more liable to fail than others, if so why?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5049a8-71b5-4aba-96b9-6c937c5e333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TO MAKE INTO A CSV AFTER IT IS ALL RAN ONCE\n",
    "df.to_csv(\"qa.query_df\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b33af-6b15-4d5e-8f41-ddecf32aceae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7687caa-33ad-40b1-9cdc-7039686d2607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68fb46-a4bf-4f86-ab01-fff5f664061f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733925b-28c3-4aff-bfb6-a03910638227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2707cb-3643-4fb4-996b-a7e42bea345d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e25af-aed7-4a45-b08c-0646b868e2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbc761-2b13-47c0-bbaf-11d53f11abe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5cd22-a4da-4d88-84c1-29f7e6e69a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5d89a-9bf9-4cb1-9908-67f65146393d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797b630-c7cb-4ca5-9fd0-69a31c6a46e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358f752-2974-4649-8505-69a023991685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4a6d7-b080-4948-8838-d6849f350844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb259025-12b2-4d92-8379-a3d968d54fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a50ae-16d2-4cf8-96f2-dbc4ce6376df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41045eff-702f-4d3f-a9be-3409474c23e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207be929-fa1a-46a5-80ce-f23b1bbef55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be949fd4-dc74-441b-830f-8b606e089a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6bd18-9bfd-49c6-a162-2501a934d945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281fbda-e7b4-4bd3-b420-49f902724b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6bc3fc-25b0-40f4-892d-32647fa7390b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634cdbd-b77f-4626-8dc3-22c1a21ee492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2c69f-9baf-405b-b985-39fb39f493e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e145f98-666c-4821-acef-1545968d940f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07955932-4dfa-464f-8c21-b94da94aa1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60951049-ea9b-4c8b-b475-22d0a201867b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpoteet/ornl/flowcept_tests/qa_chain.py:25: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = self.qa_chain({\"query\": f\"{context}. {query}\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  How many tasks are there?\n",
      "There are 3 tasks.\n",
      "---------------- I took 7.1 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = qa.ask(\" How many tasks are there?\", context = 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e2a26fd-c81b-4240-ab7c-d7a08a61ac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How many tasks are there?\n",
      "There are 3 tasks.\n",
      "---------------- I took 7.6 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: How many tasks are there?\n",
      "There are 3 tasks.\n",
      "---------------- I took 6.5 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: How many tasks are there?\n",
      "There are 3 tasks.\n",
      "---------------- I took 4.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: How many tasks are there?\n",
      "There are 3 tasks.\n",
      "---------------- I took 7.2 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: How many tasks are there?\n",
      "There are 3 tasks.\n",
      "---------------- I took 7.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "18.0\n",
      "6.693453311920166\n"
     ]
    }
   ],
   "source": [
    "result = benchmark_query(qa, \"How many tasks are there?\", use_cot=False)\n",
    "print(result[\"average_char_count\"])\n",
    "print(result[\"average_response_time_sec\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89334d9b-85df-4e40-b903-fbe3aff38313",
   "metadata": {},
   "source": [
    "#### Few-Shot \"How many tasks are there?\" with added context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ba9c867-470b-4e75-bb81-c3ffd0a1d349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "Q1: @task def a(): ... @task def b(): ... How many tasks? \n",
      "A1: 2\n",
      "\n",
      "Q2: @flowcept_task def foo(): return 1 \n",
      "@flowcept_task def bar(): return 2 \n",
      "How many tasks? \n",
      "A2: 2\n",
      "\n",
      "Q3: @flowcept_task def I_TO_H(...), H_TO_E(...), ..., DCB_TO_A(...) \n",
      "How many tasks? \n",
      "A3:\n",
      "\n",
      "A3: 8\n",
      "---------------- I took 7.2 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Q1: @task def a(): ... @task def b(): ... How many tasks? \n",
      "A1: 2\n",
      "\n",
      "Q2: @flowcept_task def foo(): return 1 \n",
      "@flowcept_task def bar(): return 2 \n",
      "How many tasks? \n",
      "A2: 2\n",
      "\n",
      "Q3: @flowcept_task def I_TO_H(...), H_TO_E(...), ..., DCB_TO_A(...) \n",
      "How many tasks? \n",
      "A3:\n",
      "\n",
      "A3: 8\n",
      "---------------- I took 6.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Q1: @task def a(): ... @task def b(): ... How many tasks? \n",
      "A1: 2\n",
      "\n",
      "Q2: @flowcept_task def foo(): return 1 \n",
      "@flowcept_task def bar(): return 2 \n",
      "How many tasks? \n",
      "A2: 2\n",
      "\n",
      "Q3: @flowcept_task def I_TO_H(...), H_TO_E(...), ..., DCB_TO_A(...) \n",
      "How many tasks? \n",
      "A3:\n",
      "\n",
      "A3: 8\n",
      "---------------- I took 6.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Q1: @task def a(): ... @task def b(): ... How many tasks? \n",
      "A1: 2\n",
      "\n",
      "Q2: @flowcept_task def foo(): return 1 \n",
      "@flowcept_task def bar(): return 2 \n",
      "How many tasks? \n",
      "A2: 2\n",
      "\n",
      "Q3: @flowcept_task def I_TO_H(...), H_TO_E(...), ..., DCB_TO_A(...) \n",
      "How many tasks? \n",
      "A3:\n",
      "\n",
      "A3: 8\n",
      "---------------- I took 6.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Q1: @task def a(): ... @task def b(): ... How many tasks? \n",
      "A1: 2\n",
      "\n",
      "Q2: @flowcept_task def foo(): return 1 \n",
      "@flowcept_task def bar(): return 2 \n",
      "How many tasks? \n",
      "A2: 2\n",
      "\n",
      "Q3: @flowcept_task def I_TO_H(...), H_TO_E(...), ..., DCB_TO_A(...) \n",
      "How many tasks? \n",
      "A3:\n",
      "\n",
      "A3: 8\n",
      "---------------- I took 6.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "5.0\n",
      "6.392284822463989\n"
     ]
    }
   ],
   "source": [
    "result = benchmark_query(qa,\"\"\"\n",
    "Q1: @task def a(): ... @task def b(): ... How many tasks? \n",
    "A1: 2\n",
    "\n",
    "Q2: @flowcept_task def foo(): return 1 \n",
    "@flowcept_task def bar(): return 2 \n",
    "How many tasks? \n",
    "A2: 2\n",
    "\n",
    "Q3: @flowcept_task def I_TO_H(...), H_TO_E(...), ..., DCB_TO_A(...) \n",
    "How many tasks? \n",
    "A3:\n",
    "\"\"\")\n",
    "print(result[\"average_char_count\"])\n",
    "print(result[\"average_response_time_sec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9abaf30e-50a4-4ce5-98fd-f4cfd5a7dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "Q1: @task def a(): ... @task def b(): ... How many tasks? \n",
      "A1: 2\n",
      "\n",
      "Q2: @flowcept_task def foo(): return 1 \n",
      "@flowcept_task def bar(): return 2 \n",
      "How many tasks? \n",
      "A2: 2\n",
      "\n",
      "Q3: @flowcept_task def I_TO_H(...), H_TO_E(...), ..., DCB_TO_A(...) \n",
      "How many tasks? \n",
      "A3:\n",
      "\n",
      "A3: 8\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = qa.ask(\"\"\"\n",
    "Q1: @task def a(): ... @task def b(): ... How many tasks? \n",
    "A1: 2\n",
    "\n",
    "Q2: @flowcept_task def foo(): return 1 \n",
    "@flowcept_task def bar(): return 2 \n",
    "How many tasks? \n",
    "A2: 2\n",
    "\n",
    "Q3: @flowcept_task def I_TO_H(...), H_TO_E(...), ..., DCB_TO_A(...) \n",
    "How many tasks? \n",
    "A3:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca6dc8-0156-42dd-9cc6-b14bb57f5de9",
   "metadata": {},
   "source": [
    "#### ReAct \"How many tasks are there?\" With context and prompting for reasoning through the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22446726-65c6-490f-b19e-42b05dea381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "You are an expert in analyzing Python workflows. Your job is to count how many tasks are present in a given code.\n",
      "\n",
      "Here is the code:\n",
      "@flowcept_task\n",
      "def I_TO_H(input_value): return (input_value * 3) + 7\n",
      "@flowcept_task\n",
      "def H_TO_E(h_value): return (h_value ** 2) / 4\n",
      "@flowcept_task\n",
      "def H_TO_F(h_value): return math.sqrt(abs(h_value)) * 6\n",
      "@flowcept_task\n",
      "def H_TO_G(h_value): return h_value - 5 if h_value > 10 else h_value + 3\n",
      "@flowcept_task\n",
      "def E_TO_D(e_value): return e_value ** 2 - 1\n",
      "@flowcept_task\n",
      "def F_TO_C(f_value): return math.log(f_value) + 7\n",
      "@flowcept_task\n",
      "def G_TO_B(g_value): return g_value ** 1.5\n",
      "@flowcept_task\n",
      "def DCB_TO_A(d, c, b): return (d + c + b) / 3\n",
      "\n",
      "Step-by-step reasoning:\n",
      "1. I will count the number of functions decorated with @flowcept_task.\n",
      "2. Each such function represents a unique task in the workflow.\n",
      "3. Let’s count them:\n",
      "   - I_TO_H\n",
      "   - H_TO_E\n",
      "   - H_TO_F\n",
      "   - H_TO_G\n",
      "   - E_TO_D\n",
      "   - F_TO_C\n",
      "   - G_TO_B\n",
      "   - DCB_TO_A\n",
      "4. \n",
      "\n",
      "Answer:\n",
      "\n",
      "There are **8** tasks in the given code.\n",
      "---------------- I took 5.9 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "react_prompt = \"\"\"\n",
    "You are an expert in analyzing Python workflows. Your job is to count how many tasks are present in a given code.\n",
    "\n",
    "Here is the code:\n",
    "@flowcept_task\n",
    "def I_TO_H(input_value): return (input_value * 3) + 7\n",
    "@flowcept_task\n",
    "def H_TO_E(h_value): return (h_value ** 2) / 4\n",
    "@flowcept_task\n",
    "def H_TO_F(h_value): return math.sqrt(abs(h_value)) * 6\n",
    "@flowcept_task\n",
    "def H_TO_G(h_value): return h_value - 5 if h_value > 10 else h_value + 3\n",
    "@flowcept_task\n",
    "def E_TO_D(e_value): return e_value ** 2 - 1\n",
    "@flowcept_task\n",
    "def F_TO_C(f_value): return math.log(f_value) + 7\n",
    "@flowcept_task\n",
    "def G_TO_B(g_value): return g_value ** 1.5\n",
    "@flowcept_task\n",
    "def DCB_TO_A(d, c, b): return (d + c + b) / 3\n",
    "\n",
    "Step-by-step reasoning:\n",
    "1. I will count the number of functions decorated with @flowcept_task.\n",
    "2. Each such function represents a unique task in the workflow.\n",
    "3. Let’s count them:\n",
    "   - I_TO_H\n",
    "   - H_TO_E\n",
    "   - H_TO_F\n",
    "   - H_TO_G\n",
    "   - E_TO_D\n",
    "   - F_TO_C\n",
    "   - G_TO_B\n",
    "   - DCB_TO_A\n",
    "4. \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "q = qa.ask(react_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02d9a3-fd84-4fe7-a689-19d724dfa567",
   "metadata": {},
   "source": [
    "## Given the final result A, what were the values of tasks D,C,and B used to compute it. Zero-Shot, Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21559dc8-17b3-4501-b45a-86b8a4d0fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "Based on the provided data, here's how to determine the values of tasks D, C, and B used to compute the final result A:\n",
      "\n",
      "* **Understanding the Data Structure:**\n",
      "\n",
      "Each document represents a task execution with details like task ID, activity ID, start/end times, and input/output values (`used` and `generated`).\n",
      "\n",
      "* **Tracing the Workflow:**\n",
      "\n",
      "1. **Identify Task A:** Look for the document with `activity_id` equal to the final activity in your workflow (likely \"A\" based on your question).\n",
      "\n",
      "2. **Find Predecessors:**  Examine the `used` field of Task A. This will contain the input values it received. These values should correspond to the outputs (`generated`) of tasks D, C, and B.\n",
      "\n",
      "3. **Locate Tasks D, C, and B:** Search for documents with `activity_id` matching \"D\", \"C\", and \"B\".  Their `generated` fields will hold the values used by Task A.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Let's say Task A's `used` field contains:\n",
      "\n",
      "```json\n",
      "{'arg_0': 10.672359527074835, 'arg_1': 234.2477321128211, 'arg_2': 71306.32753054664}\n",
      "```\n",
      "\n",
      "You would then look for tasks with `activity_id` \"D\", \"C\", and \"B\" and find their `generated` fields to match these values.\n",
      "\n",
      "**Important Notes:**\n",
      "\n",
      "* **Workflow Structure:** This assumes a linear workflow where A depends on the outputs of D, C, and B. If your workflow is more complex (e.g., parallel branches), you'll need to analyze the dependencies more carefully.\n",
      "* **Data Consistency:** Ensure that the data you're working with is complete and accurate. Missing or inconsistent data could lead to incorrect conclusions.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have the specific `activity_id` for Task A, and I can help you pinpoint the exact values for D, C, and B.\n",
      "---------------- I took 9.5 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "Based on the provided data, here's how to determine the values of tasks D, C, and B used to compute the final result A:\n",
      "\n",
      "* **Understanding the Data Structure:**\n",
      "\n",
      "Each document represents a task execution with details like task ID, activity ID, start/end times, and input/output values (`used` and `generated`).\n",
      "\n",
      "* **Tracing the Workflow:**\n",
      "\n",
      "1. **Identify Task A:** Look for the document with `activity_id` equal to the final activity in your workflow (likely \"A\" based on your question).\n",
      "\n",
      "2. **Find Predecessors:**  Examine the `used` field of Task A. This will contain the input values it received. These values should correspond to the outputs (`generated`) of tasks D, C, and B.\n",
      "\n",
      "3. **Locate Tasks D, C, and B:** Search for documents with `activity_id` matching \"D\", \"C\", and \"B\".  Their `generated` fields will hold the values used by Task A.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Let's say Task A's `used` field contains:\n",
      "\n",
      "```json\n",
      "{'arg_0': 10.672359527074835, 'arg_1': 234.2477321128211, 'arg_2': 71306.32753054664}\n",
      "```\n",
      "\n",
      "You would then look for tasks with `activity_id` \"D\", \"C\", and \"B\" and find their `generated` fields to match these values.\n",
      "\n",
      "**Important Notes:**\n",
      "\n",
      "* **Workflow Structure:** This assumes a linear workflow where A depends on the outputs of D, C, and B. If your workflow is more complex (e.g., parallel branches), you'll need to analyze the dependencies more carefully.\n",
      "* **Data Consistency:** Ensure that the data you're working with is complete and accurate. Missing or inconsistent data could lead to incorrect conclusions.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have the specific `activity_id` for Task A, and I can help you pinpoint the exact values for D, C, and B.\n",
      "---------------- I took 9.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "Based on the provided data, here's how to determine the values of tasks D, C, and B used to compute the final result A:\n",
      "\n",
      "* **Understanding the Data Structure:**\n",
      "\n",
      "Each document represents a task execution with details like task ID, activity ID, start/end times, and input/output values (`used` and `generated`).\n",
      "\n",
      "* **Tracing the Workflow:**\n",
      "\n",
      "1. **Identify Task A:** Look for the document with `activity_id` equal to the final activity in your workflow (likely \"A\" based on your question).\n",
      "\n",
      "2. **Find Predecessors:**  Examine the `used` field of Task A. This will contain the input values it received. These values should correspond to the outputs (`generated`) of tasks D, C, and B.\n",
      "\n",
      "3. **Locate Tasks D, C, and B:** Search for documents with `activity_id` matching \"D\", \"C\", and \"B\".  Their `generated` fields will hold the values used by Task A.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Let's say Task A's `used` field contains:\n",
      "\n",
      "```json\n",
      "{'arg_0': 10.672359527074835, 'arg_1': 234.2477321128211, 'arg_2': 71306.32753054664}\n",
      "```\n",
      "\n",
      "You would then look for tasks with `activity_id` \"D\", \"C\", and \"B\" and find their `generated` fields to match these values.\n",
      "\n",
      "**Important Notes:**\n",
      "\n",
      "* **Workflow Structure:** This assumes a linear workflow where A depends on the outputs of D, C, and B. If your workflow is more complex (e.g., parallel branches), you'll need to analyze the dependencies more carefully.\n",
      "* **Data Consistency:** Ensure that the data you're working with is complete and accurate. Missing or inconsistent data could lead to incorrect conclusions.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have the specific `activity_id` for Task A, and I can help you pinpoint the exact values for D, C, and B.\n",
      "---------------- I took 8.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "Based on the provided data, here's how to determine the values of tasks D, C, and B used to compute the final result A:\n",
      "\n",
      "* **Understanding the Data Structure:**\n",
      "\n",
      "Each document represents a task execution with details like task ID, activity ID, start/end times, and input/output values (`used` and `generated`).\n",
      "\n",
      "* **Tracing the Workflow:**\n",
      "\n",
      "1. **Identify Task A:** Look for the document with `activity_id` equal to the final activity in your workflow (likely \"A\" based on your question).\n",
      "\n",
      "2. **Find Predecessors:**  Examine the `used` field of Task A. This will contain the input values it received. These values should correspond to the outputs (`generated`) of tasks D, C, and B.\n",
      "\n",
      "3. **Locate Tasks D, C, and B:** Search for documents with `activity_id` matching \"D\", \"C\", and \"B\".  Their `generated` fields will hold the values used by Task A.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Let's say Task A's `used` field contains:\n",
      "\n",
      "```json\n",
      "{'arg_0': 10.672359527074835, 'arg_1': 234.2477321128211, 'arg_2': 71306.32753054664}\n",
      "```\n",
      "\n",
      "You would then look for tasks with `activity_id` \"D\", \"C\", and \"B\" and find their `generated` fields to match these values.\n",
      "\n",
      "**Important Notes:**\n",
      "\n",
      "* **Workflow Structure:** This assumes a linear workflow where A depends on the outputs of D, C, and B. If your workflow is more complex (e.g., parallel branches), you'll need to analyze the dependencies more carefully.\n",
      "* **Data Consistency:** Ensure that the data you're working with is complete and accurate. Missing or inconsistent data could lead to incorrect conclusions.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have the specific `activity_id` for Task A, and I can help you pinpoint the exact values for D, C, and B.\n",
      "---------------- I took 8.8 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "Based on the provided data, here's how to determine the values of tasks D, C, and B used to compute the final result A:\n",
      "\n",
      "* **Understanding the Data Structure:**\n",
      "\n",
      "Each document represents a task execution with details like task ID, activity ID, start/end times, and input/output values (`used` and `generated`).\n",
      "\n",
      "* **Tracing the Workflow:**\n",
      "\n",
      "1. **Identify Task A:** Look for the document with `activity_id` equal to the final activity in your workflow (likely \"A\" based on your question).\n",
      "\n",
      "2. **Find Predecessors:**  Examine the `used` field of Task A. This will contain the input values it received. These values should correspond to the outputs (`generated`) of tasks D, C, and B.\n",
      "\n",
      "3. **Locate Tasks D, C, and B:** Search for documents with `activity_id` matching \"D\", \"C\", and \"B\".  Their `generated` fields will hold the values used by Task A.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Let's say Task A's `used` field contains:\n",
      "\n",
      "```json\n",
      "{'arg_0': 10.672359527074835, 'arg_1': 234.2477321128211, 'arg_2': 71306.32753054664}\n",
      "```\n",
      "\n",
      "You would then look for tasks with `activity_id` \"D\", \"C\", and \"B\" and find their `generated` fields to match these values.\n",
      "\n",
      "**Important Notes:**\n",
      "\n",
      "* **Workflow Structure:** This assumes a linear workflow where A depends on the outputs of D, C, and B. If your workflow is more complex (e.g., parallel branches), you'll need to analyze the dependencies more carefully.\n",
      "* **Data Consistency:** Ensure that the data you're working with is complete and accurate. Missing or inconsistent data could lead to incorrect conclusions.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have the specific `activity_id` for Task A, and I can help you pinpoint the exact values for D, C, and B.\n",
      "---------------- I took 9.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "1655.0\n",
      "9.156252479553222\n"
     ]
    }
   ],
   "source": [
    "# Zero Shot\n",
    "result = benchmark_query(qa, \"Given the final result A, what were the values of tasks D, C, and B used to compute it?\")\n",
    "print(result[\"average_char_count\"])\n",
    "print(result[\"average_response_time_sec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c1c568e-a5ab-4e5f-8800-a47d965f2aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task.\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "Based on the provided data, here are the values used to compute the final result A:\n",
      "\n",
      "* **Task D:**  The value of task D is **10.672359527074835**. This is found in the `generated` section of the task with `activity_id` 'E_TO_D'.\n",
      "\n",
      "* **Task C:** The value of task C is **234.2477321128211**. This is found in the `generated` section of the task with `activity_id` 'F_TO_C'.\n",
      "\n",
      "* **Task B:** The value of task B is **38**. This is found in the `generated` section of the task with `activity_id` 'G_TO_B'. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about the data!\n",
      "---------------- I took 7.2 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task.\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "Based on the provided data, here are the values used to compute the final result A:\n",
      "\n",
      "* **Task D:**  The value of task D is **10.672359527074835**. This is found in the `generated` section of the task with `activity_id` 'E_TO_D'.\n",
      "\n",
      "* **Task C:** The value of task C is **234.2477321128211**. This is found in the `generated` section of the task with `activity_id` 'F_TO_C'.\n",
      "\n",
      "* **Task B:** The value of task B is **38**. This is found in the `generated` section of the task with `activity_id` 'G_TO_B'. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about the data!\n",
      "---------------- I took 7.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task.\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "Based on the provided data, here are the values used to compute the final result A:\n",
      "\n",
      "* **Task D:**  The value of task D is **10.672359527074835**. This is found in the `generated` section of the task with `activity_id` 'E_TO_D'.\n",
      "\n",
      "* **Task C:** The value of task C is **234.2477321128211**. This is found in the `generated` section of the task with `activity_id` 'F_TO_C'.\n",
      "\n",
      "* **Task B:** The value of task B is **38**. This is found in the `generated` section of the task with `activity_id` 'G_TO_B'. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about the data!\n",
      "---------------- I took 6.9 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task.\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "Based on the provided data, here are the values used to compute the final result A:\n",
      "\n",
      "* **Task D:**  The value of task D is **10.672359527074835**. This is found in the `generated` section of the task with `activity_id` 'E_TO_D'.\n",
      "\n",
      "* **Task C:** The value of task C is **234.2477321128211**. This is found in the `generated` section of the task with `activity_id` 'F_TO_C'.\n",
      "\n",
      "* **Task B:** The value of task B is **38**. This is found in the `generated` section of the task with `activity_id` 'G_TO_B'. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about the data!\n",
      "---------------- I took 6.9 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task.\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "Based on the provided data, here are the values used to compute the final result A:\n",
      "\n",
      "* **Task D:**  The value of task D is **10.672359527074835**. This is found in the `generated` section of the task with `activity_id` 'E_TO_D'.\n",
      "\n",
      "* **Task C:** The value of task C is **234.2477321128211**. This is found in the `generated` section of the task with `activity_id` 'F_TO_C'.\n",
      "\n",
      "* **Task B:** The value of task B is **38**. This is found in the `generated` section of the task with `activity_id` 'G_TO_B'. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about the data!\n",
      "---------------- I took 7.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "562.0\n",
      "7.041977882385254\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot\n",
    "result = benchmark_query(qa, \"\"\"This workflow uses @flowcept_task decorators to define each task. \n",
    "For example, @flowcept_task def step_one(): ... is a task.\n",
    "Workflow:\n",
    "@flowcept_task def I_TO_H(...)  \n",
    "@flowcept_task def H_TO_E(...)  \n",
    "@flowcept_task def H_TO_F(...)  \n",
    "@flowcept_task def H_TO_G(...)  \n",
    "@flowcept_task def E_TO_D(...)  \n",
    "@flowcept_task def F_TO_C(...)  \n",
    "@flowcept_task def G_TO_B(...)  \n",
    "@flowcept_task def DCB_TO_A(...)  \n",
    "Given the final result A, what were the values of tasks D, C, and B used to compute it?\"\"\")\n",
    "print(result[\"average_char_count\"])\n",
    "print(result[\"average_response_time_sec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6ee0c54c-fb84-4222-a568-d0e89fd251a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "\n",
      "Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. Workflow = workflow.py\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's break down how to find the values of tasks D, C, and B used to compute the final result A.\n",
      "\n",
      "**1. Identify the Task Being Asked About**\n",
      "\n",
      "We're looking for the values of tasks D, C, and B that contributed to the final result A.\n",
      "\n",
      "**2. Trace Inputs and Outputs**\n",
      "\n",
      "Looking at the workflow structure, we need to understand the dependencies:\n",
      "\n",
      "* **Task A** depends on the output of **Task B**.\n",
      "* **Task B** depends on the output of **Task C**.\n",
      "* **Task C** depends on the output of **Task D**.\n",
      "\n",
      "**3. Use Task Definitions to Compute Results**\n",
      "\n",
      "We need to examine the definitions of each task to understand how they process their inputs and produce outputs.\n",
      "\n",
      "* **Task D:** This task likely takes some initial input and performs a calculation. We need to find its output value.\n",
      "* **Task C:** This task takes the output of Task D as input and performs another calculation. We need to find its output value.\n",
      "* **Task B:** This task takes the output of Task C as input and performs a final calculation to produce the value of A.\n",
      "\n",
      "**4. Analyze the Provided Data**\n",
      "\n",
      "Unfortunately, the provided data snippets only give us information about the execution of individual tasks (like CPU usage, memory usage, etc.) but not the actual input and output values.\n",
      "\n",
      "**5. Conclusion**\n",
      "\n",
      "Without the specific definitions of tasks D, C, and B and their input/output values, we cannot directly calculate the values used to compute A.\n",
      "\n",
      "**To get the answer, we would need:**\n",
      "\n",
      "* **Task Definitions:** The code or logic defining what each task does.\n",
      "* **Input Values:** The initial data fed into Task D.\n",
      "* **Output Values:** The results produced by each task (D, C, B) during execution.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have more information about the task definitions or input values!\n",
      "---------------- I took 9.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. Workflow = workflow.py\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's break down how to find the values of tasks D, C, and B used to compute the final result A.\n",
      "\n",
      "**1. Identify the Task Being Asked About**\n",
      "\n",
      "We're looking for the values of tasks D, C, and B that contributed to the final result A.\n",
      "\n",
      "**2. Trace Inputs and Outputs**\n",
      "\n",
      "Looking at the workflow structure, we need to understand the dependencies:\n",
      "\n",
      "* **Task A** depends on the output of **Task B**.\n",
      "* **Task B** depends on the output of **Task C**.\n",
      "* **Task C** depends on the output of **Task D**.\n",
      "\n",
      "**3. Use Task Definitions to Compute Results**\n",
      "\n",
      "We need to examine the definitions of each task to understand how they process their inputs and produce outputs.\n",
      "\n",
      "* **Task D:** This task likely takes some initial input and performs a calculation. We need to find its output value.\n",
      "* **Task C:** This task takes the output of Task D as input and performs another calculation. We need to find its output value.\n",
      "* **Task B:** This task takes the output of Task C as input and performs a final calculation to produce the value of A.\n",
      "\n",
      "**4. Analyze the Provided Data**\n",
      "\n",
      "Unfortunately, the provided data snippets only give us information about the execution of individual tasks (like CPU usage, memory usage, etc.) but not the actual input and output values.\n",
      "\n",
      "**5. Conclusion**\n",
      "\n",
      "Without the specific definitions of tasks D, C, and B and their input/output values, we cannot directly calculate the values used to compute A.\n",
      "\n",
      "**To get the answer, we would need:**\n",
      "\n",
      "* **Task Definitions:** The code or logic defining what each task does.\n",
      "* **Input Values:** The initial data fed into Task D.\n",
      "* **Output Values:** The results produced by each task (D, C, B) during execution.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have more information about the task definitions or input values!\n",
      "---------------- I took 8.8 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. Workflow = workflow.py\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's break down how to find the values of tasks D, C, and B used to compute the final result A.\n",
      "\n",
      "**1. Identify the Task Being Asked About**\n",
      "\n",
      "We're looking for the values of tasks D, C, and B that contributed to the final result A.\n",
      "\n",
      "**2. Trace Inputs and Outputs**\n",
      "\n",
      "Looking at the workflow structure, we need to understand the dependencies:\n",
      "\n",
      "* **Task A** depends on the output of **Task B**.\n",
      "* **Task B** depends on the output of **Task C**.\n",
      "* **Task C** depends on the output of **Task D**.\n",
      "\n",
      "**3. Use Task Definitions to Compute Results**\n",
      "\n",
      "We need to examine the definitions of each task to understand how they process their inputs and produce outputs.\n",
      "\n",
      "* **Task D:** This task likely takes some initial input and performs a calculation. We need to find its output value.\n",
      "* **Task C:** This task takes the output of Task D as input and performs another calculation. We need to find its output value.\n",
      "* **Task B:** This task takes the output of Task C as input and performs a final calculation to produce the value of A.\n",
      "\n",
      "**4. Analyze the Provided Data**\n",
      "\n",
      "Unfortunately, the provided data snippets only give us information about the execution of individual tasks (like CPU usage, memory usage, etc.) but not the actual input and output values.\n",
      "\n",
      "**5. Conclusion**\n",
      "\n",
      "Without the specific definitions of tasks D, C, and B and their input/output values, we cannot directly calculate the values used to compute A.\n",
      "\n",
      "**To get the answer, we would need:**\n",
      "\n",
      "* **Task Definitions:** The code or logic defining what each task does.\n",
      "* **Input Values:** The initial data fed into Task D.\n",
      "* **Output Values:** The results produced by each task (D, C, B) during execution.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have more information about the task definitions or input values!\n",
      "---------------- I took 8.6 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. Workflow = workflow.py\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's break down how to find the values of tasks D, C, and B used to compute the final result A.\n",
      "\n",
      "**1. Identify the Task Being Asked About**\n",
      "\n",
      "We're looking for the values of tasks D, C, and B that contributed to the final result A.\n",
      "\n",
      "**2. Trace Inputs and Outputs**\n",
      "\n",
      "Looking at the workflow structure, we need to understand the dependencies:\n",
      "\n",
      "* **Task A** depends on the output of **Task B**.\n",
      "* **Task B** depends on the output of **Task C**.\n",
      "* **Task C** depends on the output of **Task D**.\n",
      "\n",
      "**3. Use Task Definitions to Compute Results**\n",
      "\n",
      "We need to examine the definitions of each task to understand how they process their inputs and produce outputs.\n",
      "\n",
      "* **Task D:** This task likely takes some initial input and performs a calculation. We need to find its output value.\n",
      "* **Task C:** This task takes the output of Task D as input and performs another calculation. We need to find its output value.\n",
      "* **Task B:** This task takes the output of Task C as input and performs a final calculation to produce the value of A.\n",
      "\n",
      "**4. Analyze the Provided Data**\n",
      "\n",
      "Unfortunately, the provided data snippets only give us information about the execution of individual tasks (like CPU usage, memory usage, etc.) but not the actual input and output values.\n",
      "\n",
      "**5. Conclusion**\n",
      "\n",
      "Without the specific definitions of tasks D, C, and B and their input/output values, we cannot directly calculate the values used to compute A.\n",
      "\n",
      "**To get the answer, we would need:**\n",
      "\n",
      "* **Task Definitions:** The code or logic defining what each task does.\n",
      "* **Input Values:** The initial data fed into Task D.\n",
      "* **Output Values:** The results produced by each task (D, C, B) during execution.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have more information about the task definitions or input values!\n",
      "---------------- I took 7.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "Given the final result A, what were the values of tasks D, C, and B used to compute it?\n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. Workflow = workflow.py\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's break down how to find the values of tasks D, C, and B used to compute the final result A.\n",
      "\n",
      "**1. Identify the Task Being Asked About**\n",
      "\n",
      "We're looking for the values of tasks D, C, and B that contributed to the final result A.\n",
      "\n",
      "**2. Trace Inputs and Outputs**\n",
      "\n",
      "Looking at the workflow structure, we need to understand the dependencies:\n",
      "\n",
      "* **Task A** depends on the output of **Task B**.\n",
      "* **Task B** depends on the output of **Task C**.\n",
      "* **Task C** depends on the output of **Task D**.\n",
      "\n",
      "**3. Use Task Definitions to Compute Results**\n",
      "\n",
      "We need to examine the definitions of each task to understand how they process their inputs and produce outputs.\n",
      "\n",
      "* **Task D:** This task likely takes some initial input and performs a calculation. We need to find its output value.\n",
      "* **Task C:** This task takes the output of Task D as input and performs another calculation. We need to find its output value.\n",
      "* **Task B:** This task takes the output of Task C as input and performs a final calculation to produce the value of A.\n",
      "\n",
      "**4. Analyze the Provided Data**\n",
      "\n",
      "Unfortunately, the provided data snippets only give us information about the execution of individual tasks (like CPU usage, memory usage, etc.) but not the actual input and output values.\n",
      "\n",
      "**5. Conclusion**\n",
      "\n",
      "Without the specific definitions of tasks D, C, and B and their input/output values, we cannot directly calculate the values used to compute A.\n",
      "\n",
      "**To get the answer, we would need:**\n",
      "\n",
      "* **Task Definitions:** The code or logic defining what each task does.\n",
      "* **Input Values:** The initial data fed into Task D.\n",
      "* **Output Values:** The results produced by each task (D, C, B) during execution.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have more information about the task definitions or input values!\n",
      "---------------- I took 8.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "1744.0\n",
      "8.565839481353759\n"
     ]
    }
   ],
   "source": [
    "# CoT\n",
    "result = benchmark_query(qa, \"Given the final result A, what were the values of tasks D, C, and B used to compute it?\", use_cot=True, context = WFC )\n",
    "print(result[\"average_char_count\"])\n",
    "print(result[\"average_response_time_sec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a72db8-0536-4823-8de6-fdfea0306f05",
   "metadata": {},
   "source": [
    "## What was the initial input I, and what task transformations were performed to arrive at output A?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a333b641-d6b3-47da-a643-b2b1548c1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What was the initial input I, and what task transformations were performed to arrive at output A?\n",
      "I can't answer that question. The provided data doesn't contain information about the initial input I or the task transformations performed to arrive at output A.\n",
      "---------------- I took 7.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: What was the initial input I, and what task transformations were performed to arrive at output A?\n",
      "I can't answer that question. The provided data doesn't contain information about the initial input I or the task transformations performed to arrive at output A.\n",
      "---------------- I took 6.3 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: What was the initial input I, and what task transformations were performed to arrive at output A?\n",
      "I can't answer that question. The provided data doesn't contain information about the initial input I or the task transformations performed to arrive at output A.\n",
      "---------------- I took 6.3 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: What was the initial input I, and what task transformations were performed to arrive at output A?\n",
      "I can't answer that question. The provided data doesn't contain information about the initial input I or the task transformations performed to arrive at output A.\n",
      "---------------- I took 6.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: What was the initial input I, and what task transformations were performed to arrive at output A?\n",
      "I can't answer that question. The provided data doesn't contain information about the initial input I or the task transformations performed to arrive at output A.\n",
      "---------------- I took 6.4 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "162.0\n",
      "6.483285999298095\n"
     ]
    }
   ],
   "source": [
    "# Zero-Shot\n",
    "result = benchmark_query(qa, \"What was the initial input I, and what task transformations were performed to arrive at output A?\")\n",
    "print(result[\"average_char_count\"])\n",
    "print(result[\"average_response_time_sec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b48c3f33-bb8f-467c-9626-f7e7ba54671e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "\n",
      "**Question:** Given final output A, what was the initial input I and what transformations were performed step-by-step to produce A?)\n",
      "Based on the provided workflow definition, here's how the transformations occur step-by-step to produce output A from initial input I:\n",
      "\n",
      "1. **I_TO_H:** The initial input `I` is processed by the task `I_TO_H`.\n",
      "\n",
      "2. **H_TO_E, H_TO_F, H_TO_G:** The output from `I_TO_H` (let's call it `H`) is then split into three parallel paths:\n",
      "   - `H_TO_E` processes `H` to produce `E`.\n",
      "   - `H_TO_F` processes `H` to produce `F`.\n",
      "   - `H_TO_G` processes `H` to produce `G`.\n",
      "\n",
      "3. **E_TO_D:** The output `E` from `H_TO_E` is further processed by `E_TO_D` to produce `D`.\n",
      "\n",
      "4. **F_TO_C:** The output `F` from `H_TO_F` is processed by `F_TO_C` to produce `C`.\n",
      "\n",
      "5. **G_TO_B:** The output `G` from `H_TO_G` is processed by `G_TO_B` to produce `B`.\n",
      "\n",
      "6. **DCB_TO_A:** Finally, the outputs `D`, `C`, and `B` are combined by the task `DCB_TO_A` to produce the final output `A`.\n",
      "\n",
      "**Therefore, the initial input I undergoes a series of transformations through the tasks I_TO_H, H_TO_E, E_TO_D, H_TO_F, F_TO_C, H_TO_G, G_TO_B, and finally DCB_TO_A to produce the final output A.**\n",
      "---------------- I took 7.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "\n",
      "**Question:** Given final output A, what was the initial input I and what transformations were performed step-by-step to produce A?)\n",
      "Based on the provided workflow definition, here's how the transformations occur step-by-step to produce output A from initial input I:\n",
      "\n",
      "1. **I_TO_H:** The initial input `I` is processed by the task `I_TO_H`.\n",
      "\n",
      "2. **H_TO_E, H_TO_F, H_TO_G:** The output from `I_TO_H` (let's call it `H`) is then split into three parallel paths:\n",
      "   - `H_TO_E` processes `H` to produce `E`.\n",
      "   - `H_TO_F` processes `H` to produce `F`.\n",
      "   - `H_TO_G` processes `H` to produce `G`.\n",
      "\n",
      "3. **E_TO_D:** The output `E` from `H_TO_E` is further processed by `E_TO_D` to produce `D`.\n",
      "\n",
      "4. **F_TO_C:** The output `F` from `H_TO_F` is processed by `F_TO_C` to produce `C`.\n",
      "\n",
      "5. **G_TO_B:** The output `G` from `H_TO_G` is processed by `G_TO_B` to produce `B`.\n",
      "\n",
      "6. **DCB_TO_A:** Finally, the outputs `D`, `C`, and `B` are combined by the task `DCB_TO_A` to produce the final output `A`.\n",
      "\n",
      "**Therefore, the initial input I undergoes a series of transformations through the tasks I_TO_H, H_TO_E, E_TO_D, H_TO_F, F_TO_C, H_TO_G, G_TO_B, and finally DCB_TO_A to produce the final output A.**\n",
      "---------------- I took 7.8 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "\n",
      "**Question:** Given final output A, what was the initial input I and what transformations were performed step-by-step to produce A?)\n",
      "Based on the provided workflow definition, here's how the transformations occur step-by-step to produce output A from initial input I:\n",
      "\n",
      "1. **I_TO_H:** The initial input `I` is processed by the task `I_TO_H`.\n",
      "\n",
      "2. **H_TO_E, H_TO_F, H_TO_G:** The output from `I_TO_H` (let's call it `H`) is then split into three parallel paths:\n",
      "   - `H_TO_E` processes `H` to produce `E`.\n",
      "   - `H_TO_F` processes `H` to produce `F`.\n",
      "   - `H_TO_G` processes `H` to produce `G`.\n",
      "\n",
      "3. **E_TO_D:** The output `E` from `H_TO_E` is further processed by `E_TO_D` to produce `D`.\n",
      "\n",
      "4. **F_TO_C:** The output `F` from `H_TO_F` is processed by `F_TO_C` to produce `C`.\n",
      "\n",
      "5. **G_TO_B:** The output `G` from `H_TO_G` is processed by `G_TO_B` to produce `B`.\n",
      "\n",
      "6. **DCB_TO_A:** Finally, the outputs `D`, `C`, and `B` are combined by the task `DCB_TO_A` to produce the final output `A`.\n",
      "\n",
      "**Therefore, the initial input I undergoes a series of transformations through the tasks I_TO_H, H_TO_E, E_TO_D, H_TO_F, F_TO_C, H_TO_G, G_TO_B, and finally DCB_TO_A to produce the final output A.**\n",
      "---------------- I took 7.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "\n",
      "**Question:** Given final output A, what was the initial input I and what transformations were performed step-by-step to produce A?)\n",
      "Based on the provided workflow definition, here's how the transformations occur step-by-step to produce output A from initial input I:\n",
      "\n",
      "1. **I_TO_H:** The initial input `I` is processed by the task `I_TO_H`.\n",
      "\n",
      "2. **H_TO_E, H_TO_F, H_TO_G:** The output from `I_TO_H` (let's call it `H`) is then split into three parallel paths:\n",
      "   - `H_TO_E` processes `H` to produce `E`.\n",
      "   - `H_TO_F` processes `H` to produce `F`.\n",
      "   - `H_TO_G` processes `H` to produce `G`.\n",
      "\n",
      "3. **E_TO_D:** The output `E` from `H_TO_E` is further processed by `E_TO_D` to produce `D`.\n",
      "\n",
      "4. **F_TO_C:** The output `F` from `H_TO_F` is processed by `F_TO_C` to produce `C`.\n",
      "\n",
      "5. **G_TO_B:** The output `G` from `H_TO_G` is processed by `G_TO_B` to produce `B`.\n",
      "\n",
      "6. **DCB_TO_A:** Finally, the outputs `D`, `C`, and `B` are combined by the task `DCB_TO_A` to produce the final output `A`.\n",
      "\n",
      "**Therefore, the initial input I undergoes a series of transformations through the tasks I_TO_H, H_TO_E, E_TO_D, H_TO_F, F_TO_C, H_TO_G, G_TO_B, and finally DCB_TO_A to produce the final output A.**\n",
      "---------------- I took 7.8 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "\n",
      "**Question:** Given final output A, what was the initial input I and what transformations were performed step-by-step to produce A?)\n",
      "Based on the provided workflow definition, here's how the transformations occur step-by-step to produce output A from initial input I:\n",
      "\n",
      "1. **I_TO_H:** The initial input `I` is processed by the task `I_TO_H`.\n",
      "\n",
      "2. **H_TO_E, H_TO_F, H_TO_G:** The output from `I_TO_H` (let's call it `H`) is then split into three parallel paths:\n",
      "   - `H_TO_E` processes `H` to produce `E`.\n",
      "   - `H_TO_F` processes `H` to produce `F`.\n",
      "   - `H_TO_G` processes `H` to produce `G`.\n",
      "\n",
      "3. **E_TO_D:** The output `E` from `H_TO_E` is further processed by `E_TO_D` to produce `D`.\n",
      "\n",
      "4. **F_TO_C:** The output `F` from `H_TO_F` is processed by `F_TO_C` to produce `C`.\n",
      "\n",
      "5. **G_TO_B:** The output `G` from `H_TO_G` is processed by `G_TO_B` to produce `B`.\n",
      "\n",
      "6. **DCB_TO_A:** Finally, the outputs `D`, `C`, and `B` are combined by the task `DCB_TO_A` to produce the final output `A`.\n",
      "\n",
      "**Therefore, the initial input I undergoes a series of transformations through the tasks I_TO_H, H_TO_E, E_TO_D, H_TO_F, F_TO_C, H_TO_G, G_TO_B, and finally DCB_TO_A to produce the final output A.**\n",
      "---------------- I took 6.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "1049.0\n",
      "7.398245143890381\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot\n",
    "result = benchmark_query(qa, \"\"\"This workflow uses @flowcept_task decorators to define each task. \n",
    "For example, @flowcept_task def step_one(): ... is a task\n",
    "Workflow:\n",
    "@flowcept_task def I_TO_H(...)  \n",
    "@flowcept_task def H_TO_E(...)  \n",
    "@flowcept_task def H_TO_F(...)  \n",
    "@flowcept_task def H_TO_G(...)  \n",
    "@flowcept_task def E_TO_D(...)  \n",
    "@flowcept_task def F_TO_C(...)  \n",
    "@flowcept_task def G_TO_B(...)  \n",
    "@flowcept_task def DCB_TO_A(...)  \n",
    "\n",
    "**Question:** Given final output A, what was the initial input I and what transformations were performed step-by-step to produce A?)\"\"\")\n",
    "print(result[\"average_char_count\"])\n",
    "print(result[\"average_response_time_sec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cdb015-e4b2-4feb-9ac0-b3f559612362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3a231-f09f-4b39-90d6-40af989e403f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea759f-4ec2-46e8-a43e-9a7acc9fe311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfb8c1-afc8-479c-97e1-db245de40248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e71c6-ef7e-4e25-b32c-4c195ae32dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf59def-23c0-4f90-95d9-80c619079063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9619e760-dc65-4ac9-bc2c-f8053ab799af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e18bac-0269-4be0-9b78-076d892f5f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb711f1-8a4e-43ae-8bbb-74914c8216cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c6161-f80d-44e9-a195-972ae386bef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9b934b9-56e1-4bd4-a339-3c019b8b50ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How do you get 8 tasks, and what are they?\n",
      "You provided 8 documents, each representing a task. \n",
      "\n",
      "Here are the tasks and their durations:\n",
      "\n",
      "1. **Task ID:** 1749217820.748001\n",
      "   - **Activity ID:** DCB_TO_A\n",
      "   - **Duration:** 2 milliseconds (ended_at - started_at)\n",
      "\n",
      "2. **Task ID:** 1749217820.7363381\n",
      "   - **Activity ID:** F_TO_C\n",
      "   - **Duration:** 2 milliseconds\n",
      "\n",
      "3. **Task ID:** 1749217820.7026298\n",
      "   - **Activity ID:** I_TO_H\n",
      "   - **Duration:** 20 milliseconds\n",
      "\n",
      "4. **Task ID:** 1749217820.7422442\n",
      "   - **Activity ID:** G_TO_B\n",
      "   - **Duration:** 3 milliseconds\n",
      "\n",
      "5. **Task ID:** 1749217820.748001\n",
      "   - **Activity ID:** DCB_TO_A\n",
      "   - **Duration:** 2 milliseconds\n",
      "\n",
      "6. **Task ID:** 1749217820.7363381\n",
      "   - **Activity ID:** F_TO_C\n",
      "   - **Duration:** 2 milliseconds\n",
      "\n",
      "7. **Task ID:** 1749217820.7026298\n",
      "   - **Activity ID:** I_TO_H\n",
      "   - **Duration:** 20 milliseconds\n",
      "\n",
      "8. **Task ID:** 1749217820.7422442\n",
      "   - **Activity ID:** G_TO_B\n",
      "   - **Duration:** 3 milliseconds \n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "---------------- I took 8.5 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = qa.ask('How do you get 8 tasks, and what are they?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0313178-46dd-4735-b468-f369ad13c976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Tell me the entire workflow, how it goes from I to A ?\n",
      "Based on the provided data, here's how the workflow progresses from I to A:\n",
      "\n",
      "1. **I_TO_H:** This task starts at `2025-06-06 13:50:20.730000` and ends at `2025-06-06 13:50:20.733000`. It has an `activity_id` of \"I_TO_H\".\n",
      "\n",
      "2. **F_TO_C:** This task starts at `2025-06-06 13:50:20.736000` and ends at `2025-06-06 13:50:20.738000`. It has an `activity_id` of \"F_TO_C\".\n",
      "\n",
      "3. **DCB_TO_A:** This task starts at `2025-06-06 13:50:20.748000` and ends at `2025-06-06 13:50:20.750000`. It has an `activity_id` of \"DCB_TO_A\".\n",
      "\n",
      "**Therefore, the workflow progresses as follows:**\n",
      "\n",
      "I_TO_H -> F_TO_C -> DCB_TO_A \n",
      "\n",
      "Let me know if you have any other questions about the workflow!\n",
      "---------------- I took 7.6 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = qa.ask('Tell me the entire workflow, how it goes from I to A ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce9c17-0588-47dd-a831-6114336a6279",
   "metadata": {},
   "source": [
    "# Which tasks were executed in parallel after task H, and what were their output values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5b0e6b2d-07f2-4c20-a69f-caf904b89e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which tasks were executed in parallel after task H, and what were their output values?\n",
      "I don't know.\n",
      "---------------- I took 7.1 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Which tasks were executed in parallel after task H, and what were their output values?\n",
      "I don't know.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Which tasks were executed in parallel after task H, and what were their output values?\n",
      "I don't know.\n",
      "---------------- I took 5.8 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Which tasks were executed in parallel after task H, and what were their output values?\n",
      "I don't know.\n",
      "---------------- I took 6.0 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: Which tasks were executed in parallel after task H, and what were their output values?\n",
      "I don't know.\n",
      "---------------- I took 5.8 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "13.0\n",
      "6.168262147903443\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot\n",
    "result = benchmark_query(qa, \"Which tasks were executed in parallel after task H, and what were their output values?\")\n",
    "print(result[\"average_char_count\"])\n",
    "print(result[\"average_response_time_sec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c19c7623-37eb-4fd1-ba68-a8073ad5fbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task.\n",
      "\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "\n",
      "Which tasks were executed in parallel after task H, and what were their output values?\n",
      "Based on the provided workflow definition, the tasks executed in parallel after task H are:\n",
      "\n",
      "* **H_TO_E**\n",
      "* **H_TO_F**\n",
      "* **H_TO_G**\n",
      "\n",
      "The output values for these tasks are:\n",
      "\n",
      "* **H_TO_E:**  `arg_0` = 39.344631145812\n",
      "* **H_TO_F:** `arg_0` = 10.672359527074835\n",
      "* **H_TO_G:** `arg_0` = 234.2477321128211\n",
      "---------------- I took 6.7 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task.\n",
      "\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "\n",
      "Which tasks were executed in parallel after task H, and what were their output values?\n",
      "Based on the provided workflow definition, the tasks executed in parallel after task H are:\n",
      "\n",
      "* **H_TO_E**\n",
      "* **H_TO_F**\n",
      "* **H_TO_G**\n",
      "\n",
      "The output values for these tasks are:\n",
      "\n",
      "* **H_TO_E:**  `arg_0` = 39.344631145812\n",
      "* **H_TO_F:** `arg_0` = 10.672359527074835\n",
      "* **H_TO_G:** `arg_0` = 234.2477321128211\n",
      "---------------- I took 5.3 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task.\n",
      "\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "\n",
      "Which tasks were executed in parallel after task H, and what were their output values?\n",
      "Based on the provided workflow definition, the tasks executed in parallel after task H are:\n",
      "\n",
      "* **H_TO_E**\n",
      "* **H_TO_F**\n",
      "* **H_TO_G**\n",
      "\n",
      "The output values for these tasks are:\n",
      "\n",
      "* **H_TO_E:**  `arg_0` = 39.344631145812\n",
      "* **H_TO_F:** `arg_0` = 10.672359527074835\n",
      "* **H_TO_G:** `arg_0` = 234.2477321128211\n",
      "---------------- I took 6.9 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task.\n",
      "\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "\n",
      "Which tasks were executed in parallel after task H, and what were their output values?\n",
      "Based on the provided workflow definition, the tasks executed in parallel after task H are:\n",
      "\n",
      "* **H_TO_E**\n",
      "* **H_TO_F**\n",
      "* **H_TO_G**\n",
      "\n",
      "The output values for these tasks are:\n",
      "\n",
      "* **H_TO_E:**  `arg_0` = 39.344631145812\n",
      "* **H_TO_F:** `arg_0` = 10.672359527074835\n",
      "* **H_TO_G:** `arg_0` = 234.2477321128211\n",
      "---------------- I took 6.5 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "Q: This workflow uses @flowcept_task decorators to define each task. \n",
      "For example, @flowcept_task def step_one(): ... is a task.\n",
      "\n",
      "Workflow:\n",
      "@flowcept_task def I_TO_H(...)  \n",
      "@flowcept_task def H_TO_E(...)  \n",
      "@flowcept_task def H_TO_F(...)  \n",
      "@flowcept_task def H_TO_G(...)  \n",
      "@flowcept_task def E_TO_D(...)  \n",
      "@flowcept_task def F_TO_C(...)  \n",
      "@flowcept_task def G_TO_B(...)  \n",
      "@flowcept_task def DCB_TO_A(...)  \n",
      "\n",
      "Which tasks were executed in parallel after task H, and what were their output values?\n",
      "Based on the provided workflow definition, the tasks executed in parallel after task H are:\n",
      "\n",
      "* **H_TO_E**\n",
      "* **H_TO_F**\n",
      "* **H_TO_G**\n",
      "\n",
      "The output values for these tasks are:\n",
      "\n",
      "* **H_TO_E:**  `arg_0` = 39.344631145812\n",
      "* **H_TO_F:** `arg_0` = 10.672359527074835\n",
      "* **H_TO_G:** `arg_0` = 234.2477321128211\n",
      "---------------- I took 6.5 s to answer this.\n",
      "\n",
      "\n",
      "\n",
      "298.0\n",
      "6.396313571929932\n"
     ]
    }
   ],
   "source": [
    "# Few-shot\n",
    "result = benchmark_query(qa,\"\"\"This workflow uses @flowcept_task decorators to define each task. \n",
    "For example, @flowcept_task def I_TO_H(input_value): ... is a task.\n",
    "\n",
    "Workflow:\n",
    "@flowcept_task def I_TO_H(...)  \n",
    "@flowcept_task def H_TO_E(...)  \n",
    "@flowcept_task def H_TO_F(...)  \n",
    "@flowcept_task def H_TO_G(...)  \n",
    "@flowcept_task def E_TO_D(...)  \n",
    "@flowcept_task def F_TO_C(...)  \n",
    "@flowcept_task def G_TO_B(...)  \n",
    "@flowcept_task def DCB_TO_A(...)  \n",
    "\n",
    "Which tasks were executed in parallel after task H, and what were their output values?\"\"\")\n",
    "print(result[\"average_char_count\"])\n",
    "print(result[\"average_response_time_sec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec6322-14e5-4b1b-85fe-702465054e5f",
   "metadata": {},
   "source": [
    "# CoT Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9786e01e-2c13-45c2-b64b-5b9bb36b3189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "<function Workflow.run at 0x7fa34e8cc2c0>\n",
      "Track, trace, and provide the H_TO_G task and input.\n",
      "\n",
      "You are analyzing a mathematic workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. Workflow = workflow.py\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's break down the task you're asking about and trace its inputs and outputs.\n",
      "\n",
      "**1. Identify the task:**\n",
      "\n",
      "You're asking about the `H_TO_G` task.\n",
      "\n",
      "**2. Trace its inputs:**\n",
      "\n",
      "Looking at the provided data, we can see that the `H_TO_G` task has an input `arg_0` with a value of `43`.  \n",
      "\n",
      "**3. Trace its outputs:**\n",
      "\n",
      "The `H_TO_G` task generates an output `arg_0` with a value of `39.344631145812`.\n",
      "\n",
      "**4. Use task definitions to compute or infer results:**\n",
      "\n",
      "Unfortunately, we don't have the definitions of the tasks themselves. We only have the input and output values.  \n",
      "\n",
      "**5. Workflow failure:**\n",
      "\n",
      "Since the workflow seems to have completed successfully (all tasks have a `status` of `FINISHED`), we don't need to worry about identifying the last successful task or checking for errors.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The `H_TO_G` task takes an input of `43` and produces an output of `39.344631145812`. Without the task definitions, we can't determine the exact computation or transformation that occurred.\n",
      "---------------- I took 6.3 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qcot = qa.ask(cot_prompt(\"Track, trace, and provide the H_TO_G task and input.\", context = WFC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ca208f5-7957-4234-a355-fcdfe1541704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "None\n",
      "Track, trace, and provide the H_TO_G task and input.\n",
      "\n",
      "You are analyzing a workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. Workflow = workflow.py\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's break down the problem and find the H_TO_G task and its input.\n",
      "\n",
      "1. **Identify the Task:** We're looking for the task named \"H_TO_G\".\n",
      "\n",
      "2. **Trace its Inputs:** To find the input for H_TO_G, we need to look for the task that directly precedes it in the workflow.  \n",
      "\n",
      "   * Looking at the provided data, we see a task named \"F_TO_C\". This suggests a sequence where \"F_TO_C\" likely produces output that is consumed by \"H_TO_G\".\n",
      "\n",
      "3. **Trace its Outputs:**  The output of \"H_TO_G\" would be passed to the next task in the workflow sequence. However, the provided data doesn't give us information about the tasks that come after \"H_TO_G\".\n",
      "\n",
      "4. **Use Task Definitions:** We don't have the definitions of the tasks (like workflow.py) to know exactly how they process data.  \n",
      "\n",
      "5. **Workflow Failure:** Since the provided data doesn't indicate any workflow failure, we can assume \"H_TO_G\" executed successfully.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Based on the task names and the typical flow of data in workflows, the input for the \"H_TO_G\" task is likely the output produced by the \"F_TO_C\" task. \n",
      "\n",
      "**Important Note:** Without the actual task definitions (workflow.py), we can't know the specific data format or content being passed between tasks.\n",
      "---------------- I took 51.6 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qcot = qa.ask(cot_prompt(\"Track, trace, and provide the H_TO_G task and input.\", context = 'None'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa00bbbe-d357-4053-8f9b-948857142838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "<function Workflow.run at 0x7fd408c10400>\n",
      "Was 'E_TO_D' apart of a sequential or branched path?\n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. Workflow = workflow.py\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's analyze the workflow to determine if 'E_TO_D' was part of a sequential or branched path.\n",
      "\n",
      "1. **Identify the Task:** We are looking for the task named 'E_TO_D'.\n",
      "\n",
      "2. **Trace its Inputs:** To understand if 'E_TO_D' is sequential or branched, we need to know what task(s) it depends on.  \n",
      "\n",
      "3. **Trace its Outputs:** We also need to see what task(s) depend on 'E_TO_D'.\n",
      "\n",
      "4. **Workflow Structure:** Without the actual workflow definition (workflow.py), we can't definitively say if 'E_TO_D' is sequential or branched. \n",
      "\n",
      "   * **Sequential:** If 'E_TO_D' has only one input task and only one output task, it's likely part of a sequential path.\n",
      "   * **Branched:** If 'E_TO_D' has multiple input tasks or multiple output tasks, it's likely part of a branched path.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Without the workflow definition, we can't conclusively determine if 'E_TO_D' was part of a sequential or branched path. We need to see the dependencies between tasks to make that judgment.\n",
      "---------------- I took 17.5 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qcot = qa.ask(cot_prompt(\"Was 'E_TO_D' apart of a sequential or branched path?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dba3dc2f-1377-49ce-8d46-3b2d81107038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "<function Workflow.run at 0x7fd408c10400>\n",
      "Was 'E_TO_D' apart of a sequential or branched path?\n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. Workflow = WFC, which is in your context parameter.\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's analyze the workflow to determine if 'E_TO_D' was part of a sequential or branched path.\n",
      "\n",
      "1. **Identify the Task:** We are looking at the task 'E_TO_D'.\n",
      "\n",
      "2. **Trace its Inputs:** To understand if 'E_TO_D' is sequential or branched, we need to know its dependencies.  We need to find the task that produced the input for 'E_TO_D'.\n",
      "\n",
      "3. **Trace its Outputs:** We also need to see what task consumes the output of 'E_TO_D'.\n",
      "\n",
      "4. **Workflow Structure:**  Without the full workflow definition (WFC), we can't definitively say if 'E_TO_D' is sequential or branched.  \n",
      "\n",
      "   * **Sequential:** If 'E_TO_D' has only one input task and only one output task, it's likely part of a sequential path.\n",
      "   * **Branched:** If 'E_TO_D' has multiple input tasks or multiple output tasks, it's likely part of a branched path.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Based on the provided information, we cannot conclusively determine if 'E_TO_D' was part of a sequential or branched path. We need more information about the workflow structure (WFC) to trace its dependencies and outputs.\n",
      "---------------- I took 32.7 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qcot = qa.ask(cot_prompt(\"Was 'E_TO_D' apart of a sequential or branched path?\", context = WFC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a5517b4-0ddf-4d1b-a893-6247137e26d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      " I_TO_H is the entry point, branching into three parallel paths: H_TO_E, H_TO_F, and H_TO_G. These produce inputs for E_TO_D, F_TO_C, and G_TO_B respectively, which then converge into a final aggregation function DCB_TO_A.  \n",
      "Was 'E_TO_D' apart of a sequential or branched path?\n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. The entire workflow is stored in 'WFC', which is in your context parameter.\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's analyze the workflow to determine if 'E_TO_D' was part of a sequential or branched path.\n",
      "\n",
      "1. **Identify the Task:** The task in question is 'E_TO_D'.\n",
      "\n",
      "2. **Trace its Inputs:**  The task name 'E_TO_D' suggests it takes input from a task named 'E' and produces output for a task named 'D'.\n",
      "\n",
      "3. **Trace its Outputs:**  'E_TO_D' feeds its output to 'D'.\n",
      "\n",
      "4. **Workflow Structure:**\n",
      "\n",
      "   - The description mentions 'I_TO_H' as the entry point, branching into three parallel paths: 'H_TO_E', 'H_TO_F', and 'H_TO_G'.\n",
      "   - 'E_TO_D', 'F_TO_C', and 'G_TO_B' are mentioned as tasks that converge into a final aggregation function 'DCB_TO_A'.\n",
      "\n",
      "5. **Conclusion:**  \n",
      "\n",
      "   Since 'E_TO_D' is one of the tasks that converge into the final aggregation, and it receives input from 'E' (which is part of a parallel branch), we can conclude that 'E_TO_D' is part of a **branched path**. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about this workflow!\n",
      "---------------- I took 16.0 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qcot = qa.ask(cot_prompt(\"Was 'E_TO_D' apart of a sequential or branched path?\", context = WFS ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c3f1549-188b-4c3d-a363-0ca8a6242bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WFS = \" I_TO_H is the entry point, branching into three parallel paths: H_TO_E, H_TO_F, and H_TO_G. These produce inputs for E_TO_D, F_TO_C, and G_TO_B respectively, which then converge into a final aggregation function DCB_TO_A.  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bacfd6bb-e4bb-4e19-b26e-6e56724d9f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "\n",
      " How is 'G_TO_B' related to 'I_TO_H' in the control-flow path? \n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. The entire workflow is stored in 'WFC', which is in your context parameter.\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's analyze the relationship between 'G_TO_B' and 'I_TO_H' in the control-flow path.\n",
      "\n",
      "1. **Identify the tasks:** We are looking at 'G_TO_B' and 'I_TO_H'.\n",
      "\n",
      "2. **Trace inputs and outputs:**\n",
      "\n",
      "   * **'G_TO_B':** This task name suggests it takes input from a task named 'G' and produces output for a task named 'B'.\n",
      "   * **'I_TO_H':** This task takes input from 'I' and produces output for 'H'.\n",
      "\n",
      "3. **Analyze the workflow structure:**\n",
      "\n",
      "   * The names imply a sequence: 'I_TO_H' likely comes before 'G_TO_B' because 'H' is a potential input for 'G'.\n",
      "\n",
      "4. **Conclusion:**\n",
      "\n",
      "   Based on the naming convention, 'I_TO_H' is likely a **predecessor** of 'G_TO_B' in the control-flow path. Data flows from 'I' to 'H', then from 'H' (possibly through other tasks) to 'G', and finally from 'G' to 'B'. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about the workflow!\n",
      "---------------- I took 43.5 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qcot = qa.ask(cot_prompt(\" How is 'G_TO_B' related to 'I_TO_H' in the control-flow path? \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdee8c6e-ca22-48d0-8c43-bcfb2c605edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      " I_TO_H is the entry point, branching into three parallel paths: H_TO_E, H_TO_F, and H_TO_G. These produce inputs for E_TO_D, F_TO_C, and G_TO_B respectively, which then converge into a final aggregation function DCB_TO_A.  \n",
      " How is 'G_TO_B' related to 'I_TO_H' in the control-flow path? \n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. The entire workflow is stored in 'WFC', which is in your context parameter.\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's analyze the relationship between 'G_TO_B' and 'I_TO_H' in the control-flow path:\n",
      "\n",
      "1. **Identify the Task:** We are looking at the task 'G_TO_B'.\n",
      "\n",
      "2. **Trace its Inputs:** The task name 'G_TO_B' suggests it takes input from a task named 'G' and produces output for a task named 'B'.\n",
      "\n",
      "3. **Trace its Outputs:**  'G_TO_B' feeds its output to 'B'.\n",
      "\n",
      "4. **Workflow Structure:** The description states that 'I_TO_H' is the entry point and branches into three parallel paths. 'G_TO_B' is one of the paths stemming from 'I_TO_H'.\n",
      "\n",
      "5. **Relationship:**  Since 'G_TO_B' is a direct branch from 'I_TO_H', we can conclude that 'G_TO_B' is a **direct successor** of 'I_TO_H' in the control-flow path. \n",
      "\n",
      "\n",
      "**Answer:** 'G_TO_B' is a direct successor of 'I_TO_H' in the control-flow path.\n",
      "---------------- I took 46.8 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qcot = qa.ask(cot_prompt(\" How is 'G_TO_B' related to 'I_TO_H' in the control-flow path? \", context = WFS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "929a805e-4c7c-4fc0-82b6-81082807bcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "\n",
      " You are an expert in HPC workflow provenance analysis with a deep knowledge of data lineage tracing, workflow management, and computing systems. \n",
      "                        You are analyzing provenance data from a complex workflow consisting of multiple tasks.\n",
      "                        Workflow provenance in HPC refers to a comprehensive way of tracking and recording all computational processes, transformations, resource utilization, and dependencies within a workflow.\n",
      "                        Within the workflow, a task object has its own task provenance. \n",
      "                        Input data is stored in the 'used' field whereas the created output data is stored in the 'generated' field. Tasks with the same 'workflow_id' indicate that they belong to the same workflow execution trace.\n",
      "                        Your job is to understand the complex workflow in detail so that you can accurately respond to the query.\n",
      "\n",
      "                        How is 'G_TO_B' related to 'I_TO_H' in the control-flow path? \n",
      "\n",
      "You are analyzing a mathematical workflow composed of multiple tasks. \n",
      "Each task processes data and passes it to the next task. \n",
      "Tasks have names like I_TO_H and H_TO_G. Data flows through tasks in a directed graph based on dependencies. The entire workflow is stored in 'WFC', which is in your context parameter.\n",
      "\n",
      "To answer questions, follow these reasoning steps:\n",
      "1. Identify the task being asked about.\n",
      "2. Trace its inputs: which task produced the data it consumed?\n",
      "3. Trace its outputs: what does it return and where does it go?\n",
      "4. Use task definitions to compute or infer results.\n",
      "5. If the workflow failed, identify the last successful task, then check inputs and errors in the failed task.\n",
      "\n",
      "Always explain your reasoning step by step before providing a final answer.\n",
      "\n",
      "Let's break down the relationship between 'G_TO_B' and 'I_TO_H' in the control-flow path.\n",
      "\n",
      "1. **Identify the Tasks:**\n",
      "\n",
      "   - We're looking at two tasks: 'G_TO_B' and 'I_TO_H'.\n",
      "\n",
      "2. **Trace Inputs and Outputs:**\n",
      "\n",
      "   - **'G_TO_B'**: This task likely takes input from a task named 'G' (since it's 'G_TO_B') and produces output that goes to task 'B'.\n",
      "   - **'I_TO_H'**: This task takes input from 'I' and produces output that goes to 'H'.\n",
      "\n",
      "3. **Control-Flow Path:**\n",
      "\n",
      "   - The question asks about the *control-flow* path. This means we're interested in the order of task execution, not just data flow.\n",
      "\n",
      "   -  Since 'I_TO_H' produces output for 'H', and 'G_TO_B' takes input from 'G', we need to determine if 'H' and 'G' are related.\n",
      "\n",
      "4. **Workflow Structure:**\n",
      "\n",
      "   - Without the full workflow definition ('WFC'), we can't definitively say how 'H' and 'G' are connected. However, based on the naming convention, it's plausible that 'H' is a predecessor of 'G' in the workflow. This would mean 'I_TO_H' executes before 'G_TO_B'.\n",
      "\n",
      "**Possible Relationship:**\n",
      "\n",
      "- **Sequential Execution:** If 'H' is a direct predecessor of 'G', then 'I_TO_H' would execute before 'G_TO_B' in a sequential manner.\n",
      "\n",
      "**Important Note:**\n",
      "\n",
      "- Without the complete workflow structure ('WFC'), this is an educated guess based on naming conventions. The actual relationship could be more complex, involving parallel execution or other dependencies.\n",
      "---------------- I took 248.1 s to answer this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qcot = qa.ask(cot_prompt(\"\"\" You are an expert in HPC workflow provenance analysis with a deep knowledge of data lineage tracing, workflow management, and computing systems. \n",
    "                        You are analyzing provenance data from a complex workflow consisting of multiple tasks.\n",
    "                        Workflow provenance in HPC refers to a comprehensive way of tracking and recording all computational processes, transformations, resource utilization, and dependencies within a workflow.\n",
    "                        Within the workflow, a task object has its own task provenance. \n",
    "                        Input data is stored in the 'used' field whereas the created output data is stored in the 'generated' field. Tasks with the same 'workflow_id' indicate that they belong to the same workflow execution trace.\n",
    "                        Your job is to understand the complex workflow in detail so that you can accurately respond to the query.\n",
    "                        \n",
    "                        How is 'G_TO_B' related to 'I_TO_H' in the control-flow path? \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049693f-ee6a-4edf-a741-244c22570d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
