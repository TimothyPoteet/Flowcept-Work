{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313aeed6-75d5-4724-b956-9f0f788b3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowcept import Flowcept\n",
    "from workflow import Workflow\n",
    "from qa_chain import QAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77c0cc-798f-415b-9c55-978662a40ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_id = Workflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90372f-c4bf-4f66-aaa0-cee72e9b253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_id = 'x'\n",
    "qa = QAChain().build(workflow_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633924b0-abd1-4b2d-9f43-240a3e46d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Runs Code\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def cot_prompt(query):\n",
    "    \"\"\"Add Chain of Thought prompting to a query\"\"\" \n",
    "    return f\"Let's think step by step. {query} Please explain your reasoning process.\"\n",
    "\n",
    "def benchmark_query_with_tracking(qa_chain, base_query_id, query, runs=5, use_cot=False, context=None):\n",
    "    \"\"\"\n",
    "    Run multiple queries and automatically track them in the QAChain DataFrame\n",
    "    \n",
    "    Args:\n",
    "        qa_chain: Your QAChain instance\n",
    "        base_query_id: Base ID like \"DF-DL-Q01-NS-NC-R3-Swallow\" (R3 sets starting run number)\n",
    "        query: The actual query text\n",
    "        runs: Number of times to run the query\n",
    "        use_cot: Whether to use Chain of Thought prompting\n",
    "        context: Custom context (if None, uses default based on query_id)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    prompt = cot_prompt(query) if use_cot else query\n",
    "    \n",
    "    # MODIFY QUERY ON COT \n",
    "    if use_cot and \"NS\" in base_query_id:\n",
    "        base_query_id = base_query_id.replace(\"NS\", \"CoT\")\n",
    "    \n",
    "    # EXTRACT STARTING ITERATION NUM\n",
    "    run_match = re.search(r'-R(\\d+)$', base_query_id)  # Match run number only if at end of string\n",
    "    if run_match:\n",
    "        start_run = int(run_match.group(1))\n",
    "        # REMOVE RUN SUFFIX ONLY AT END\n",
    "        base_id_clean = re.sub(r'-R\\d+$', '', base_query_id)\n",
    "    else:\n",
    "        start_run = 1\n",
    "        base_id_clean = base_query_id\n",
    "    \n",
    "    for i in range(runs):\n",
    "        current_run = start_run + i\n",
    "        query_id = f\"{base_id_clean}-R{current_run:02d}\"\n",
    "        \n",
    "       \n",
    "        if context:\n",
    "            result = qa_chain.ask(prompt, query_id=query_id, context=context)\n",
    "        else:\n",
    "            result = qa_chain.ask(prompt, query_id=query_id)\n",
    "        \n",
    "        # RESPONSE AND TIMING INFO\n",
    "        response = result[\"result\"]\n",
    "        query_row = qa_chain.query_df[qa_chain.query_df['Query_ID'] == query_id].iloc[-1]\n",
    "        response_time = query_row['Response_Time']\n",
    "        char_count = query_row['Response_Chars']\n",
    "        \n",
    "        results.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"run\": current_run,\n",
    "            \"response_time_sec\": response_time,\n",
    "            \"response\": response,\n",
    "            \"char_count\": char_count\n",
    "        })\n",
    "    \n",
    "    avg_time = sum(r[\"response_time_sec\"] for r in results) / runs\n",
    "    avg_char_count = sum(r[\"char_count\"] for r in results) / runs\n",
    "    \n",
    "    return {\n",
    "        \"base_query_id\": base_query_id,\n",
    "        \"query\": query,\n",
    "        \"prompt_used\": prompt,\n",
    "        \"use_cot\": use_cot,\n",
    "        \"runs\": results,\n",
    "        \"average_response_time_sec\": avg_time,\n",
    "        \"average_char_count\": avg_char_count\n",
    "    }\n",
    "\n",
    "def run_query_suite(qa_chain, query_suite):\n",
    "    \"\"\"\n",
    "    Run a suite of queries with different configurations\n",
    "    \n",
    "    Args:\n",
    "        qa_chain: Your QAChain instance\n",
    "        query_suite: List of dictionaries with query configurations\n",
    "                    Each dict should have: base_id, query, runs, use_cot, context\n",
    "    \n",
    "    Example:\n",
    "        suite = [\n",
    "            {\n",
    "                \"base_id\": \"DF-DL-Q01-NS-NC-R1-Swallow\",\n",
    "                \"query\": \"What is the data lineage?\",\n",
    "                \"runs\": 3,\n",
    "                \"use_cot\": False,\n",
    "                \"context\": None\n",
    "            },\n",
    "            {\n",
    "                \"base_id\": \"CF-EO-Q02-CoT-FC-R5-Swallow\", \n",
    "                \"query\": \"What is the execution order?\",\n",
    "                \"runs\": 3,\n",
    "                \"use_cot\": True,\n",
    "                \"context\": \"X\"\n",
    "            }\n",
    "        ]\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for config in query_suite:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Running: {config['base_id']}\")\n",
    "        print(f\"Query: {config['query']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = benchmark_query_with_tracking(\n",
    "            qa_chain=qa_chain,\n",
    "            base_query_id=config[\"base_id\"],\n",
    "            query=config[\"query\"],\n",
    "            runs=config.get(\"runs\", 5),\n",
    "            use_cot=config.get(\"use_cot\", False),\n",
    "            context=config.get(\"context\", None)\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        print(f\"Average Response Time: {result['average_response_time_sec']:.2f}s\")\n",
    "        print(f\"Average Character Count: {result['average_char_count']:.0f}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def generate_query_ids_for_batch(base_id, runs, start_run=1):\n",
    "    \"\"\"\n",
    "    Helper function to generate query IDs for batch accuracy updates\n",
    "    \n",
    "    Args:\n",
    "        base_id: Base query ID (without run numbers)\n",
    "        runs: Number of runs\n",
    "        start_run: Starting run number\n",
    "    \n",
    "    Returns:\n",
    "        List of query IDs\n",
    "    \"\"\"\n",
    "    base_id_clean = re.sub(r'-R\\d+$', '', base_id)\n",
    "    \n",
    "    query_ids = []\n",
    "    for i in range(runs):\n",
    "        current_run = start_run + i\n",
    "        query_id = f\"{base_id_clean}-R{current_run:02d}\"\n",
    "        query_ids.append(query_id)\n",
    "    \n",
    "    return query_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79d132-d210-49c3-9980-671b191453e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of DF to import into csv after it runs, could just make it a csv, but for now that is what Im doing.\n",
    "# DF TO INSTANCE\n",
    "qa.query_df = pd.DataFrame(columns=[\n",
    "    'Query_ID', 'Query_Text', 'Query_Chars', 'Response_Text', \n",
    "    'Response_Chars', 'Response_Time', 'Accuracy'\n",
    "])\n",
    "\n",
    "# METHODS\n",
    "def update_accuracy(self, query_id, accuracy_score):\n",
    "    mask = self.query_df['Query_ID'] == query_id\n",
    "    if mask.any():\n",
    "        self.query_df.loc[mask, 'Accuracy'] = accuracy_score\n",
    "        print(f\"Updated accuracy for {query_id}: {accuracy_score}\")\n",
    "    else:\n",
    "        print(f\"Query ID {query_id} not found\")\n",
    "\n",
    "def export_queries(self, filename=\"query_results.csv\"):\n",
    "    self.query_df.to_csv(filename, index=False)\n",
    "    print(f\"Query results exported to {filename}\")\n",
    "\n",
    "# METHODS IN INSTANCE\n",
    "import types\n",
    "qa.update_accuracy = types.MethodType(update_accuracy, qa)\n",
    "qa.export_queries = types.MethodType(export_queries, qa)\n",
    "\n",
    "# TEST\n",
    "print(qa.query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd5d86-b19d-420a-b10a-920046e3ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa.ask AND what is being tracked into DF\n",
    "import types\n",
    "\n",
    "def ask(self, query, query_id=None, context=None):\n",
    "    \"\"\"\n",
    "    Main ask method that can optionally take a query_id parameter\n",
    "    If query_id is not provided, auto-generates one\n",
    "    \"\"\"\n",
    "    # If no query_id provided, auto-generate one\n",
    "    if query_id is None:\n",
    "        query_id = f\"Q_{len(self.query_df) + 1}\"\n",
    "    \n",
    "    if context is None:\n",
    "        context = \"Each document represents a task. All tasks belong to a same workflow execution trace. \"\n",
    "        context += \"The time the task started is stored in the started_at. The time the task ended is stored in the ended_at. The task duration is ended_at - started_at for each task \"\n",
    "    \n",
    "    # Prepare full query text\n",
    "    full_query = f\"{context}. {query}\"\n",
    "    \n",
    "    # Time the query\n",
    "    from time import time\n",
    "    t0 = time()\n",
    "    result = self.qa_chain({\"query\": full_query})\n",
    "    response_time = time() - t0\n",
    "    \n",
    "    # Extract response text\n",
    "    response_text = result[\"result\"]\n",
    "    \n",
    "    # Calculate character counts\n",
    "    query_chars = len(query)\n",
    "    response_chars = len(response_text)\n",
    "    \n",
    "    # Add to tracking DataFrame\n",
    "    import pandas as pd\n",
    "    new_row = {\n",
    "        'Query_ID': query_id,\n",
    "        'Query_Text': query,\n",
    "        'Query_Chars': query_chars,\n",
    "        'Response_Text': response_text,\n",
    "        'Response_Chars': response_chars,\n",
    "        'Response_Time': response_time,\n",
    "        'Accuracy': None  # To be filled manually\n",
    "    }\n",
    "    \n",
    "    self.query_df = pd.concat([self.query_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    print(f\"Q: {query}\")\n",
    "    print(response_text)\n",
    "    print(f\"---------------- I took {response_time:.1f} s to answer this.\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Replace the ask method on your existing instance\n",
    "qa.ask = types.MethodType(ask, qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9782046-2303-4db2-ad21-9ebf6d2bcdf0",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9288db-7edf-40b1-93a6-244e5780c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_suite = [\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q01-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What tasks take the value 'H' as input?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q02-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Was the output of I_TO_H used directly by multiple downstream tasks? If so what tasks?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q03-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Which tasks produce intermediate values that are consumed by other tasks?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q04-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How many values are combined to produce the final output 'A'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q05-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"Did any task produce more than one output?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q06-NS-LC-R1-Swallow\", \n",
    "        \"query\": \"What is the data type of the output produced by 'I_TO_H'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q07-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"What are the output values of 'E_TO_D', 'F_TO_C', and 'G_TO_B'?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False, \n",
    "        \"context\": None\n",
    "    },\n",
    "    {\n",
    "        \"base_id\": \"DF-INOP-Q08-NS-LC-R1-Swallow\",\n",
    "        \"query\": \"How can 'H_TO_G's output change given the input?\",\n",
    "        \"runs\": 3,\n",
    "        \"use_cot\": False,\n",
    "        \"context\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all queries in the suite\n",
    "all_results = run_query_suite(qa, query_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f637d5-58a1-46b2-bc36-b5e537ec40d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0267f-0e3a-4aed-a124-ae29ba921be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a4d50-94ae-42cd-859d-4dc296c97c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a673acee-6e91-4a11-af07-431a5c8dbb61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c117c-a1d4-4ac4-add7-9730d3e5ec81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40dc7a-db44-4c6a-bf0a-010d72e8b439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7444ae3-8198-47e0-9c56-26a6f11c7632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
